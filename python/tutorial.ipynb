{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import bisect\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "from itertools import chain\n",
    "from math import pi\n",
    "from sklearn import preprocessing\n",
    "from GGLasso.gglasso.problem import glasso_problem\n",
    "from utils import transform_features, scale_array_by_diagonal\n",
    "from utils import PCA\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from networkx.utils import cuthill_mckee_ordering\n",
    "\n",
    "from bokeh.io import output_notebook, show, save\n",
    "from bokeh.models import Range1d, Circle, ColumnDataSource, MultiLine, HoverTool, LabelSet, PointDrawTool\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.plotting import from_networkx\n",
    "from bokeh.palettes import RdBu, Blues8\n",
    "from bokeh.models import HoverTool, Panel, Tabs, ColorBar, LinearColorMapper\n",
    "from bokeh.layouts import row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(X, L, inverse=True):\n",
    "    sig, V = np.linalg.eigh(L)\n",
    "\n",
    "    # sort eigenvalues in descending order\n",
    "    sig = sig[::-1]\n",
    "    V = V[:, ::-1]\n",
    "\n",
    "    ind = np.argwhere(sig > 1e-9)\n",
    "\n",
    "    if inverse:\n",
    "        loadings = V[:, ind] @ np.diag(np.sqrt(1 / sig[ind]))\n",
    "    else:\n",
    "        loadings = V[:, ind] @ np.diag(np.sqrt(sig[ind]))\n",
    "\n",
    "    # compute the projection\n",
    "    zu = X.values @ loadings\n",
    "\n",
    "    return zu, loadings, np.round(sig[ind].squeeze(), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_heatmap(data: pd.DataFrame(), title: str = None, labels_dict: dict=None, labels_dict_reversed: dict=None,\n",
    "                  width: int = 1500, height: int = 1500, label_size: str = \"5pt\", not_low_rank: bool = True):\n",
    "    nlabels = len(labels_dict)\n",
    "    df = data.iloc[::-1] # rotate matrix 90 degrees\n",
    "    df = pd.DataFrame(df.stack(), columns=['covariance']).reset_index()\n",
    "    df.columns = [\"taxa_y\", \"taxa_x\", \"covariance\"]\n",
    "    if not_low_rank:\n",
    "        df = df.replace({\"taxa_x\": labels_dict, \"taxa_y\": labels_dict})\n",
    "\n",
    "    color_list, colors = _get_colors(df=df)\n",
    "    mapper = LinearColorMapper(palette=colors, low=-1, high=1)\n",
    "    color_bar = ColorBar(color_mapper=mapper, location=(0, 0))\n",
    "\n",
    "    bottom, top, left, right = _get_bounds(nlabels=nlabels)\n",
    "\n",
    "    source = ColumnDataSource(dict(top=top, bottom=bottom, left=left, right=right, color_list=color_list,\n",
    "                                   taxa_x=df['taxa_x'], taxa_y=df['taxa_y'], covariance=df['covariance']))\n",
    "\n",
    "    bokeh_tools = [\"save, zoom_in, zoom_out, wheel_zoom, box_zoom, crosshair, reset, hover\"]\n",
    "\n",
    "    p = figure(plot_width=width, plot_height=height, x_range=(0, nlabels), y_range=(0, nlabels),\n",
    "               title=title, title_location='above', x_axis_location=\"below\",\n",
    "               tools=bokeh_tools, toolbar_location='left')\n",
    "\n",
    "    p.quad(top=\"top\", bottom=\"bottom\", left=\"left\", right=\"right\", line_color='white', color=\"color_list\",\n",
    "           source=source)\n",
    "    p.xaxis.major_label_orientation = pi / 4\n",
    "    p.yaxis.major_label_orientation = \"horizontal\"\n",
    "    p.title.text_font_size = \"24pt\"\n",
    "    p.add_layout(color_bar, 'right')\n",
    "    p.toolbar.autohide = True\n",
    "\n",
    "    p.xaxis.ticker = list(range(0, nlabels))\n",
    "    p.yaxis.ticker = list(range(0, nlabels))\n",
    "    if not_low_rank:\n",
    "        p.xaxis.major_label_overrides = labels_dict\n",
    "        p.yaxis.major_label_overrides = labels_dict_reversed\n",
    "    p.xaxis.major_label_text_font_size = label_size\n",
    "    p.yaxis.major_label_text_font_size = label_size\n",
    "\n",
    "    hover = p.select(dict(type=HoverTool))\n",
    "    hover.tooltips = [\n",
    "        (\"taxa_x\", \"@taxa_x\"),\n",
    "        (\"taxa_y\", \"@taxa_y\"),\n",
    "        (\"covariance\", \"@covariance\"),\n",
    "    ]\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_network(G, title, width, height, node_size=None):\n",
    "    #Establish which categories will appear when hovering over each node\n",
    "    HOVER_TOOLTIPS = [(\"Character\", \"@index\")]\n",
    "    hover = HoverTool(tooltips=[('','@index')])\n",
    "    tools = [\"save, zoom_in, zoom_out, wheel_zoom, box_zoom, crosshair, reset, hover, pan\"]\n",
    "\n",
    "    #Create a plot â€” set dimensions, toolbar, and title\n",
    "    plot = figure(tooltips = HOVER_TOOLTIPS, plot_width=width, plot_height=height,\n",
    "                  tools=tools, active_scroll='wheel_zoom',\n",
    "                x_range=Range1d(-10.1, 10.1), \n",
    "                  y_range=Range1d(-10.1, 10.1), title=title)\n",
    "    \n",
    "    \n",
    "    \n",
    "    color_map = [\"#88CCEE\" if \"ASV\" in j else \"#DDCC77\" for j in G.nodes()] #green for bugs, and blue for covariates\n",
    "    nx.set_node_attributes(G, {j: {'color': color_map[i]} for i, j in enumerate(G.nodes())})\n",
    "\n",
    "    if node_size is not None:\n",
    "        n_degrees = {k: 15*v for k,v in G.degree()} \n",
    "        nx.set_node_attributes(G, n_degrees, 'node_size')\n",
    "        node_size = 'node_size'\n",
    "    else:\n",
    "        node_size = 40\n",
    "\n",
    "    network_graph = from_networkx(G, nx.spring_layout, scale=10, center=(0, 0))\n",
    "\n",
    "\n",
    "    #Set node size and color\n",
    "    network_graph.node_renderer.glyph = Circle(size=node_size,  fill_color=\"color\")\n",
    "    \n",
    "    #Set edge width and color\n",
    "    network_graph.edge_renderer.data_source.data[\"line_width\"] = [G.get_edge_data(a,b)['covariance']*2 for a, b in G.edges()]  ### amplify edges strengh\n",
    "    network_graph.edge_renderer.data_source.data[\"line_color\"] = [\"#117733\" if G.get_edge_data(a, b)['covariance'] >= 0 else \"#CC6677\" for a, b in G.edges()]\n",
    "    network_graph.edge_renderer.glyph.line_width = {'field': 'line_width'}\n",
    "    network_graph.edge_renderer.glyph.line_color = {'field': 'line_color'}\n",
    "\n",
    "    #Add network graph to the plot\n",
    "    plot.renderers.append(network_graph)\n",
    "    \n",
    "    x, y = zip(*network_graph.layout_provider.graph_layout.values())\n",
    "    node_labels = list(G.nodes)\n",
    "    source = ColumnDataSource({'x': x, 'y': y, 'asv': [node_labels[i] for i in range(len(x))]})\n",
    "    labels = LabelSet(x='x', y='y', text='asv', x_offset=30, y_offset=-15, source=source, render_mode='canvas', text_font_size='12pt')\n",
    "\n",
    "    plot.renderers.append(labels)    \n",
    "\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_corr(corr_array, inplace=False):\n",
    "    \"\"\"\n",
    "    Rearranges the correlation matrix, corr_array, so that groups of highly \n",
    "    correlated variables are next to eachother \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    corr_array : pandas.DataFrame or numpy.ndarray\n",
    "        a NxN correlation matrix \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame or numpy.ndarray\n",
    "        a NxN correlation matrix with the columns and rows rearranged\n",
    "    \"\"\"\n",
    "    pairwise_distances = sch.distance.pdist(corr_array)\n",
    "    linkage = sch.linkage(pairwise_distances, method='complete')\n",
    "    cluster_distance_threshold = pairwise_distances.max()/2\n",
    "    idx_to_cluster_array = sch.fcluster(linkage, cluster_distance_threshold, \n",
    "                                        criterion='distance')\n",
    "    idx = np.argsort(idx_to_cluster_array)\n",
    "    \n",
    "    if not inplace:\n",
    "        corr_array = corr_array.copy()\n",
    "    \n",
    "    if isinstance(corr_array, pd.DataFrame):\n",
    "        return corr_array.iloc[idx, :].T.iloc[idx, :]\n",
    "    return corr_array[idx, :][:, idx]\n",
    "\n",
    "\n",
    "# fig = px.imshow(-1*cluster_corr(precision_SGL), color_continuous_scale='RdBu_r', zmin=-1, zmax=1)\n",
    "# fig.update_layout(margin = dict(t=100,r=100,b=100,l=100), width = 1000, height = 1000,\n",
    "#                  title='Clustered Estimated inverse covariance: ASVs', title_x=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(corr_matrix: pd.DataFrame(), threshold: float):\n",
    "    #take the upper part only\n",
    "    upper = np.triu(np.ones(corr_matrix.shape)).astype(bool)\n",
    "    df = corr_matrix.where(upper)\n",
    "    df = pd.DataFrame(corr_matrix.stack(), columns=['covariance']).reset_index()\n",
    "    df.columns = [\"source\", \"target\", \"covariance\"]\n",
    "    \n",
    "    #remove diagonal entries\n",
    "    #df = df[df['covariance'] <= threshold]\n",
    "    df = df[abs(df['covariance']) >= threshold]\n",
    "    #remove diagonal entries\n",
    "    df = df[df['source'] != df['target']]\n",
    "    #remove zero entries\n",
    "    df = df[df['covariance'] != 0]\n",
    "    \n",
    "    #build graph\n",
    "    G = nx.from_pandas_edgelist(df, edge_attr=\"covariance\")\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_covariates(counts=pd.DataFrame(), metadata=pd.DataFrame(), L=np.ndarray, y=str):\n",
    "    proj, loadings, eigv = PCA(counts.dropna(), L, inverse=True)\n",
    "    r = np.linalg.matrix_rank(L)\n",
    "    eigv_sum = np.sum(eigv)\n",
    "    var_exp = [(value / eigv_sum) for value in sorted(eigv, reverse=True)]\n",
    "    \n",
    "    depth = pd.DataFrame(data=raw.sum(axis=0), columns=[\"sequencing depth\"])\n",
    "    metadata = depth.join(metadata)\n",
    "    \n",
    "    pc_columns = list('PC{0} ({1}%)'.format(i+1, str(100 * var_exp[i])[:4]) for i in range(0, r))\n",
    "    df_proj = pd.DataFrame(proj, columns=pc_columns, index=counts.index)\n",
    "    df = df_proj.join(metadata)\n",
    "    \n",
    "    varName1 = 'PC1 ({0}%)'.format(str(100 * var_exp[0])[:4])\n",
    "    varName2 = y\n",
    "    df['x'] = df[varName1]\n",
    "    df['y'] = df[varName2]\n",
    "\n",
    "    source = ColumnDataSource(df)\n",
    "\n",
    "    p0 = figure(tools='save, zoom_in, zoom_out, wheel_zoom, box_zoom, reset', plot_width=800, plot_height=800,\n",
    "                active_scroll=\"wheel_zoom\",\n",
    "                x_axis_label=varName1, y_axis_label=varName2,\n",
    "                tooltips=[(varName1, \"@\" + varName1),\n",
    "                          (varName2, \"@\" + varName2)\n",
    "                          ],\n",
    "                title=varName1 + \" vs \" + varName2)\n",
    "\n",
    "    exp_cmap = LinearColorMapper(palette=Blues8[::-1], low=min(df['sequencing depth'].values), high=max(df['sequencing depth'].values))\n",
    "    p0.circle('x', 'y', source=source, size=15, line_color=None, fill_color={\"field\": \"sequencing depth\", \"transform\": exp_cmap}, fill_alpha=0.3)\n",
    "\n",
    "    color_bar_plot = figure(title='sequencing depth', title_location=\"right\",\n",
    "                            height=500, width=150, toolbar_location=None, min_border=0,\n",
    "                            outline_line_color=None)\n",
    "\n",
    "    bar = ColorBar(color_mapper=exp_cmap, location=(1, 1))\n",
    "\n",
    "    color_bar_plot.add_layout(bar, 'right')\n",
    "    color_bar_plot.title.align = \"center\"\n",
    "    color_bar_plot.title.text_font_size = '12pt'\n",
    "\n",
    "    layout = row(p0, color_bar_plot)\n",
    "\n",
    "    return layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels(df):\n",
    "    i = 1\n",
    "    for col in df.columns:\n",
    "        # length of ASVs identifier\n",
    "        if len(col) == 32:\n",
    "            asv_name = \"ASV_{0}\".format(i)\n",
    "            id_dict[asv_name] = col\n",
    "            df.rename(columns={col: asv_name}, inplace=True)\n",
    "\n",
    "            i += 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_bounds(nlabels: int):\n",
    "    bottom = list(chain.from_iterable([[ii] * nlabels for ii in range(nlabels)]))\n",
    "    top = list(chain.from_iterable([[ii + 1] * nlabels for ii in range(nlabels)]))\n",
    "    left = list(chain.from_iterable([list(range(nlabels)) for ii in range(nlabels)]))\n",
    "    right = list(chain.from_iterable([list(range(1, nlabels + 1)) for ii in range(nlabels)]))\n",
    "\n",
    "    return bottom, top, left, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_colors(df: pd.DataFrame(), n_colors: int = 9):\n",
    "    colors = list(RdBu[n_colors])\n",
    "    ccorr = np.arange(-1, 1, 1 / (len(colors) / 2))\n",
    "    color_list = []\n",
    "    for value in df.covariance.values:\n",
    "        ind = bisect.bisect_left(ccorr, value) # smart array insertion\n",
    "        if ind == 0: # avoid ind == -1 on the next step\n",
    "            ind = ind + 1\n",
    "        color_list.append(colors[ind-1])\n",
    "    return color_list, colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_dict(df):\n",
    "    n_labels = len(df.columns)\n",
    "    labels_dict = dict(zip(range(n_labels), df.columns))\n",
    "    labels_dict_reversed = dict(zip(range(n_labels),list(labels_dict.values())[::-1]))\n",
    "    \n",
    "    return labels_dict, labels_dict_reversed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scater_plot(x, y, width=800, height=600, size=3):\n",
    "    bokeh_tools = [\"save, zoom_in, zoom_out, wheel_zoom, box_zoom, crosshair, reset, hover\"]\n",
    "    p = figure(plot_width=width, plot_height=height, tools=bokeh_tools, toolbar_location='left')\n",
    "\n",
    "    source = ColumnDataSource({'x': x, 'y': y})\n",
    "\n",
    "    p.circle(\"x\", \"y\", size=size, source=source, line_color=None)\n",
    "\n",
    "    p.xaxis.axis_label = x.name\n",
    "    p.yaxis.axis_label = y.name\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count table\n",
    "raw = pd.read_csv('data/composition_feature-table.tsv', sep='\\t', index_col = 0)\n",
    "\n",
    "# pattern = r'(\\w+\\.\\d+\\.\\d+)' \n",
    "# columns = [col for col in raw.columns if re.match(pattern, col)]\n",
    "\n",
    "# for prefix in set(col[:-4] for col in columns):\n",
    "#     pref1 = f\"{prefix}.1.1\"\n",
    "#     pref2 = f\"{prefix}.1.2\"\n",
    "#     pref3 = f\"{prefix}.1.3\"\n",
    "#     pref_avg = f\"{prefix}.agg\"\n",
    "\n",
    "#     if pref1 in raw.columns and pref2 in raw.columns and pref3 in raw.columns:\n",
    "#         # create a boolean mask to identify rows where pref1 has value zero and pref2 or pref3 has non-zero\n",
    "#         mask = (raw[pref1] == 0) & ((raw[pref2] != 0) | (raw[pref3] != 0))\n",
    "        \n",
    "#         # create a new column pref_avg with default value np.nan\n",
    "#         raw[pref_avg] = 0\n",
    "\n",
    "#         # assign the maximum value among pref2 and pref3 to pref_avg on rows where the mask is True\n",
    "#         raw.loc[mask, pref_avg] = raw.loc[mask, [pref2, pref3]].max(axis=1)\n",
    "        \n",
    "#         # drop pref1, pref2, and pref3\n",
    "#         raw.drop([pref1, pref2, pref3], axis=1, inplace=True)\n",
    "\n",
    "#     elif pref2 in raw.columns and pref3 in raw.columns:\n",
    "#         # create a boolean mask to identify rows where pref2 has value zero and pref3 has non-zero\n",
    "#         mask = (raw[pref2] == 0) & (raw[pref3] != 0)\n",
    "\n",
    "#         # create a new column pref_avg with default value np.nan\n",
    "#         raw[pref_avg] = 0\n",
    "\n",
    "#         # assign the value of pref3 to pref_avg on rows where the mask is True\n",
    "#         raw.loc[mask, pref_avg] = raw.loc[mask, pref3]\n",
    "        \n",
    "#         # drop pref2 and pref3\n",
    "#         raw.drop([pref2, pref3], axis=1, inplace=True)\n",
    "\n",
    "# # rename YUN1005.1.1 to YUN1005.1\n",
    "# raw.rename(columns={\"YUN1005.1.1\": \"YUN1005.agg\"}, inplace=True)\n",
    "\n",
    "print(\"Some columns contain only zeros:\", (raw == 0).all().any())\n",
    "\n",
    "\n",
    "# # calculate percentage of zeros for each row\n",
    "percentage_zeros = (raw == 0).sum(axis=1) / len(raw.columns) * 100\n",
    "\n",
    "\n",
    "# drop rows with more than 80% of zeros\n",
    "raw_filt = raw[percentage_zeros <= 80]\n",
    "print(\"Some columns contain only zeros:\", (raw_filt == 0).all().any())\n",
    "\n",
    "zero_cols = raw_filt.columns[(raw_filt == 0).all()]\n",
    "print(\"{0} samples are dropped since there is no variance\". format(zero_cols))\n",
    "raw_filt = raw_filt.drop(zero_cols, axis=1)\n",
    "print(\"Some columns contain only zeros:\", (raw_filt == 0).all().any())\n",
    "# print(percentage_zeros.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxa = pd.read_csv(str(\"~/q2-gglasso/data/atacama-taxa/taxonomy.tsv\"), index_col=0, sep='\\t')\n",
    "\n",
    "taxonomy_levels = {\"domain\": '^d__', \n",
    "                   \"phylum\": '^p__', \n",
    "                   \"class\": '^c__', \n",
    "                   \"order\": '^o__', \n",
    "                   \"family\": '^f__',\n",
    "                   \"genus\": '^g__', \n",
    "                   \"species\": '^s__'\n",
    "                  }\n",
    "\n",
    "# split taxonomic ranks in different columns\n",
    "taxa_sep = taxa['Taxon'].str.split(';', expand=True)\n",
    "\n",
    "#rename taxonomic ranks with full names\n",
    "taxa_sep.columns = taxonomy_levels.keys()\n",
    "\n",
    "# drop missing species\n",
    "taxa_sep = taxa_sep[taxa_sep.species.notnull()]\n",
    "\n",
    "# remove blank spaces from taxonomic ranks\n",
    "taxa_sep[taxa_sep.columns] = taxa_sep.apply(lambda x: x.str.strip())\n",
    "\n",
    "taxa_sep.shape\n",
    "\n",
    "\n",
    "taxa_dict = dict()\n",
    "\n",
    "for level in taxa_sep.columns:\n",
    "    df_level = raw.join(taxa_sep[level])\n",
    "    df_level = df_level.groupby(level).sum()\n",
    "    \n",
    "    taxa_dict[level] = df_level\n",
    "    \n",
    "taxa_dict.keys()\n",
    "\n",
    "phylum = taxa_dict['phylum']\n",
    "print(\"Some columns contain only zeros:\", (phylum == 0).all().any())\n",
    "\n",
    "zero_cols = phylum.columns[(phylum == 0).all()]\n",
    "print(\"{0} samples are dropped since there is no variance\". format(zero_cols))\n",
    "# select columns containing only zeros\n",
    "zero_df = phylum[zero_cols]\n",
    "\n",
    "phylum = phylum.drop(zero_cols, axis=1)\n",
    "\n",
    "print(\"Some columns contain only zeros:\", (phylum == 0).all().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clr-transformation\n",
    "# clr = transform_features(raw, transformation=\"mclr\")\n",
    "# clr = transform_features(phylum, transformation=\"mclr\")\n",
    "clr = transform_features(raw_filt, transformation=\"mclr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv('data/acm_meta.tsv', sep='\\t', index_col = 0)\n",
    "\n",
    "# select only numeric features\n",
    "meta = meta.loc[:, meta.iloc[0, :] != 'categorical']\n",
    "meta = meta.apply(pd.to_numeric, errors='coerce')\n",
    "# drop QIIME2 header\n",
    "meta = meta.iloc[1:]\n",
    "# fill missing values with zeros\n",
    "meta = meta.fillna(0)\n",
    "\n",
    "# meta_agg = meta.loc[columns]\n",
    "\n",
    "# # Group the rows by the first part of the index\n",
    "# grouped = meta_agg.groupby(meta_agg.index.str.split('.').str[0])\n",
    "\n",
    "# # Calculate the mean of each group\n",
    "# means = grouped.mean()\n",
    "\n",
    "# # # Create a new DataFrame with the means and the old indices\n",
    "# meta_agg = pd.DataFrame(means.values, index=means.index, columns=means.columns)\n",
    "# meta_agg.index = meta_agg.index + '.agg'\n",
    "\n",
    "# meta  = pd.concat([meta, meta_agg])\n",
    "\n",
    "#scale data\n",
    "scaler = preprocessing.StandardScaler().fit(meta)\n",
    "meta_scaled = scaler.transform(meta)\n",
    "meta_scaled = pd.DataFrame(meta_scaled, index=meta.index, columns=meta.columns)\n",
    "\n",
    "# transpose count data\n",
    "clr_T = clr.T\n",
    "# join by sample id\n",
    "df = clr_T.join(meta_scaled)\n",
    "\n",
    "# Rename long feature IDs with concise names\n",
    "vis_df = df.copy()\n",
    "id_dict = dict()\n",
    "vis_df = add_labels(vis_df)\n",
    "\n",
    "#calculate covariance\n",
    "n_cov = meta_scaled.shape[1]\n",
    "asv = df.iloc[:, :-n_cov]\n",
    "S = np.cov(asv.T.values, bias=True)\n",
    "\n",
    "# correlation between ASVs ONLY\n",
    "corr = scale_array_by_diagonal(S)\n",
    "\n",
    "#add labels\n",
    "asv_names = vis_df.iloc[:, :-n_cov].columns\n",
    "vis_S = pd.DataFrame(corr, columns=asv_names, index=asv_names)\n",
    "\n",
    "# # correlation between ASVs and covariates\n",
    "S_meta = np.cov(df.T.values, bias=True)\n",
    "corr_meta = scale_array_by_diagonal(S_meta)\n",
    "vis_S_meta = pd.DataFrame(corr_meta, columns=vis_df.columns, index=vis_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 1500\n",
    "height = 1500\n",
    "label_size = \"8pt\"\n",
    "lables_0, re_labels_0 = create_label_dict(vis_S)\n",
    "\n",
    "p0 = _make_heatmap(data=vis_S, labels_dict=lables_0, labels_dict_reversed=re_labels_0,\n",
    "                       title=\"Correlation: ASVs\", width=width, height=height,\n",
    "                       label_size=label_size)\n",
    "\n",
    "meta_corr = vis_S_meta.iloc[-n_cov:, -n_cov:]\n",
    "lables_1, re_labels_1 = create_label_dict(meta_corr)\n",
    "\n",
    "p1 = _make_heatmap(data=meta_corr, labels_dict=lables_1, labels_dict_reversed=re_labels_1,\n",
    "                       title=\"Correlation: covariates\", width=width, height=height,\n",
    "                       label_size=label_size)\n",
    "\n",
    "# drop highly correlated covariates\n",
    "hcorr_cov = ['relative-humidity-soil-high', 'relative-humidity-soil-low', 'percent-relative-humidity-soil-100', 'temperature-soil-high', 'temperature-soil-low']\n",
    "\n",
    "for frame in [vis_S_meta, df, vis_df]:\n",
    "    frame.drop(hcorr_cov, axis=1, inplace=True)\n",
    "    frame.rename(columns={'average-soil-relative-humidity':'average humidity','average-soil-temperature': 'average temperature',}, inplace=True)\n",
    "\n",
    "vis_S_meta = vis_S_meta.T\n",
    "vis_S_meta.drop(hcorr_cov, axis=1, inplace=True)\n",
    "vis_S_meta.rename(columns={'average-soil-relative-humidity':'average humidity','average-soil-temperature': 'average temperature',}, inplace=True)\n",
    "\n",
    "n_cov = df.shape[1] - asv.shape[1]\n",
    "lables_2, re_labels_2 = create_label_dict(vis_S_meta)\n",
    "\n",
    "p2 = _make_heatmap(data=vis_S_meta, labels_dict=lables_2, labels_dict_reversed=re_labels_2,\n",
    "                       title=\"Correlation: ASVs + covariates\", width=width, height=height,\n",
    "                       label_size=label_size)\n",
    "\n",
    "show(p0)\n",
    "show(p1)\n",
    "show(p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = asv.shape[0]\n",
    "p = asv.shape[1]\n",
    "print(\"Shape of data without covariates: {0}, {1}\".format(N, p))\n",
    "\n",
    "N_meta = df.shape[0]\n",
    "p_meta = df.shape[1]\n",
    "print(\"Shape of data with covariates: {0}, {1}\".format(N_meta, p_meta))\n",
    "\n",
    "#hyperparameters\n",
    "# lambda1_range = np.logspace(0, -4, 15)\n",
    "# mu1_range = np.logspace(0.9, 0.4, 10)\n",
    "# lambda1_range = np.logspace(-2, -4, 20)\n",
    "# mu1_range = np.logspace(-2, -4, 20)\n",
    "lambda1_range = np.logspace(0, -1, 15)\n",
    "mu1_range = np.logspace(-0.5, -4, 10)\n",
    "modelselect_params = {'lambda1_range': lambda1_range, 'mu1_range': mu1_range}\n",
    "\n",
    "P_SGL = glasso_problem(corr, N, latent=False, do_scaling=False)\n",
    "P_SGL.model_selection(modelselect_params=modelselect_params, method='eBIC', gamma=0.1)\n",
    "\n",
    "P_SGL_low = glasso_problem(corr, N, latent=True, do_scaling=False)\n",
    "P_SGL_low.model_selection(modelselect_params=modelselect_params, method='eBIC', gamma=0.1)\n",
    "\n",
    "# create lambda matrix full of zeros\n",
    "shape_meta = (p_meta, p_meta)\n",
    "mask = np.zeros(shape_meta)\n",
    "# add small constant, so ADMM could converge\n",
    "mask = mask + 0.01\n",
    "# heavy penalize species\n",
    "n_bugs = len(asv.columns)\n",
    "bugs_block = np.ones((n_bugs, n_bugs))\n",
    "mask[0:n_bugs, 0:n_bugs] += bugs_block - 0.01\n",
    "lambda1_mask_exp = mask\n",
    "df_mask_exp = pd.DataFrame(lambda1_mask_exp, columns=vis_df.columns, index=vis_df.columns)\n",
    "\n",
    "modelselect_params[\"lambda1_mask\"] = lambda1_mask_exp\n",
    "P_SGL_adapt = glasso_problem(vis_S_meta.values, N_meta, latent=False, do_scaling=False)\n",
    "P_SGL_adapt.model_selection(modelselect_params=modelselect_params, method='eBIC', gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SGL solution with lambda={lambda1} and mu={mu1}\".format(**P_SGL.reg_params))\n",
    "print(\"Adaptive SGL+low-rank solution with lambda={lambda1} and mu={mu1}\".format(**P_SGL_adapt.reg_params))\n",
    "print(\"SGL+low-rank solution with lambda={lambda1} and mu={mu1}\".format(**P_SGL_low.reg_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_SGL_low.modelselect_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 1500\n",
    "height = 1500\n",
    "label_size = \"8pt\"\n",
    "\n",
    "# for visualization reasons we transform inverse covaraince to negative inverse covaraince, i.e., multiply by -1\n",
    "sgl = -1 * pd.DataFrame(P_SGL.solution.precision_, columns=asv_names, index=asv_names)\n",
    "adapt = -1 * pd.DataFrame(P_SGL_adapt.solution.precision_, columns=vis_df.columns, index=vis_df.columns)\n",
    "low = -1 * pd.DataFrame(P_SGL_low.solution.precision_, columns=asv_names, index=asv_names)\n",
    "\n",
    "\n",
    "lables_sgl, re_labels_sgl = create_label_dict(sgl)\n",
    "lables_adapt, re_labels_adapt = create_label_dict(adapt)\n",
    "lables_low, re_labels_low = create_label_dict(low)\n",
    "\n",
    "p_sgl = _make_heatmap(data=sgl, labels_dict=lables_sgl, labels_dict_reversed=re_labels_sgl,\n",
    "                       title=\"SGL estimated (negative) inverse covariance\", width=width, height=height,\n",
    "                       label_size=label_size)\n",
    "\n",
    "p_adapt = _make_heatmap(data=adapt, labels_dict=lables_adapt, labels_dict_reversed=re_labels_adapt,\n",
    "                       title=\"Adaptive estimated (negative) inverse covariance\", width=width, height=height,\n",
    "                       label_size=label_size)\n",
    "\n",
    "p_low = _make_heatmap(data=low, labels_dict=lables_low, labels_dict_reversed=re_labels_low,\n",
    "                       title=\"SGL+low-rank estimated (negative) inverse covariance\", width=width, height=height,\n",
    "                       label_size=label_size)\n",
    "show(p_sgl)\n",
    "show(p_adapt)\n",
    "show(p_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_cols = list(adapt.iloc[:, -n_cov:].columns)\n",
    "# asv18_edges = [\"ASV_18\", \"ASV_51\", \"ASV_46\", \"ASV_13\", \"ASV_7\", \"ASV_5\"]\n",
    "# asv18_51 =[\"ASV_18\", \"ASV_51\"]\n",
    "# asv18_edges_adapt = meta_cols + asv18_51\n",
    "\n",
    "# sgl_edges = sgl[sgl.columns.intersection(asv18_edges)].loc[asv18_edges]\n",
    "# adapt_edges = adapt[adapt.columns.intersection(asv18_edges_adapt)].loc[asv18_51]\n",
    "# low_edges = low[low.columns.intersection(asv18_edges)].loc[asv18_edges]\n",
    "\n",
    "\n",
    "# G_SGL = create_graph(sgl_edges, threshold=0.1)\n",
    "# G_adapt = create_graph(adapt_edges, threshold=0.1)\n",
    "# G_low = create_graph(low_edges, threshold=0.1)\n",
    "\n",
    "\n",
    "G_SGL = create_graph(sgl, threshold=0.0001)\n",
    "G_adapt = create_graph(adapt, threshold=0.0001)\n",
    "G_low = create_graph(low, threshold=0.0001)\n",
    "\n",
    "\n",
    "width, height= 1000, 1000\n",
    "\n",
    "network_sgl = plot_network(G_SGL, title=\"SGL\", height=height, width=width)\n",
    "network_adapt = plot_network(G_adapt, title=\"Adaptive\",  height=height, width=width)\n",
    "network_low = plot_network(G_low, title=\"Low-rank\",  height=height, width=width)\n",
    "\n",
    "show(network_sgl)\n",
    "show(network_adapt)\n",
    "show(network_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_18_51 = scater_plot(vis_df[\"ASV_1\"], vis_df[\"ASV_10\"])\n",
    "# p_18_temp = scater_plot(vis_df[\"p__Gemmatimonadota\"], vis_df[\"ph\"])\n",
    "# p_51_temp = scater_plot(vis_df[\"p__Actinobacteriota\"], vis_df[\"ph\"])\n",
    "\n",
    "show(p_18_51)\n",
    "# show(p_18_temp)\n",
    "# show(p_51_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_cov = adapt.iloc[:-n_cov, -n_cov:]\n",
    "\n",
    "L_adapt = inv_cov @ inv_cov.T\n",
    "L_adapt.shape\n",
    "\n",
    "L_1 = pd.DataFrame(P_SGL_low.solution.lowrank_, columns=asv_names, index=asv_names)\n",
    "L_2 = pd.DataFrame(L_adapt, columns=asv_names, index=asv_names)\n",
    "\n",
    "r1 = np.linalg.matrix_rank(L_1)\n",
    "r2 = np.linalg.matrix_rank(L_2)\n",
    "\n",
    "print(\"L1-rank: {0}\".format(r1))\n",
    "print(\"L2-rank: {0}\".format(r2))\n",
    "\n",
    "proj_1, loadings_1, eigv_1 = PCA(asv, L_1, inverse=True)\n",
    "\n",
    "eigv_sum_1 = np.sum(eigv_1)\n",
    "var_exp_1 = [(value / eigv_sum_1) for value in sorted(eigv_1, reverse=True)]\n",
    "\n",
    "proj_2, loadings_2, eigv_2 = PCA(asv, L_2, inverse=True)\n",
    "\n",
    "eigv_sum_2 = np.sum(eigv_2)\n",
    "var_exp_2 = [(value / eigv_sum_2) for value in sorted(eigv_2, reverse=True)]\n",
    "\n",
    "pca_plot = project_covariates(asv, metadata=meta_scaled, L=L_1, y='average-soil-temperature')\n",
    "show(pca_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 1500\n",
    "height = 1500\n",
    "label_size = \"8pt\"\n",
    "\n",
    "adapt_theta = adapt.copy()\n",
    "\n",
    "asv_cov = adapt_theta.iloc[:-n_cov, -n_cov:]\n",
    "\n",
    "l1_norm = np.linalg.norm(asv_cov.values, axis=1)\n",
    "\n",
    "adapt_theta['l1'] = np.append(l1_norm, np.zeros(n_cov))\n",
    "\n",
    "adapt_theta = adapt_theta.T\n",
    "\n",
    "adapt_theta['l1'] = np.append(l1_norm, np.zeros(n_cov+1))\n",
    "adapt_theta = adapt_theta.sort_values(by=['l1'], ascending=False)\n",
    "adapt_theta = adapt_theta.T\n",
    "adapt_theta = adapt_theta.sort_values(by=['l1'], ascending=False)\n",
    "\n",
    "lables_l1, re_labels_l1 = create_label_dict(adapt_theta)\n",
    "\n",
    "p_l1 = _make_heatmap(data=adapt_theta, labels_dict=lables_l1, labels_dict_reversed=re_labels_l1,\n",
    "                       title=\"Esatimated inverse covariance sorted by l1-norm of the covariates\", width=width, height=height,\n",
    "                       label_size=label_size)\n",
    "show(p_l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_components = pd.DataFrame(loadings_1, index=low.index)\n",
    "pc_components = pc_components.iloc[::-1]\n",
    "pc_components.columns = [\"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\", \"PC6\"]\n",
    "\n",
    "# low-rank solution: r1=6\n",
    "identity = pd.DataFrame(np.eye(r1, r1), index=pc_components.columns, columns = pc_components.columns)\n",
    "# PCs are linearly independent by the definition\n",
    "pc_columns = pd.concat([pc_components, identity], axis=0)\n",
    "\n",
    "# inverse cov matrix extended by PCs\n",
    "asv_pc = pd.concat([low, pc_components], axis=1)\n",
    "asv_pc = pd.concat([asv_pc.T, pc_columns], axis=1)\n",
    "\n",
    "asv_low = asv_pc.iloc[:-r1, -r1:]\n",
    "# l1-norm of partial correlation between ASVs and PCs\n",
    "l1_norm_pc = np.linalg.norm(asv_low.values, axis=1)\n",
    "\n",
    "asv_pc['l1'] = np.append(l1_norm_pc, np.zeros(r1))\n",
    "asv_pc = asv_pc.T\n",
    "asv_pc['l1'] = np.append(l1_norm_pc, np.zeros(r1 + 1))\n",
    "\n",
    "#sorting by the order of adaptive l1-norm sorted solution\n",
    "n_asvs = len(vis_S)\n",
    "sorted_asv = asv_pc.iloc[:n_asvs, :].reindex(index=adapt_theta.iloc[:n_asvs, :].index)\n",
    "sorted_asv_pc = sorted_asv.T.join(asv_pc.iloc[:, -7:])\n",
    "sorted_asv = sorted_asv_pc.iloc[:n_asvs, :].reindex(index=adapt_theta.iloc[:n_asvs, :].index)\n",
    "sorted_l1_low = pd.concat([sorted_asv, sorted_asv_pc.iloc[n_asvs:, :]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lables_l1_low, re_labels_l1_low = create_label_dict(sorted_l1_low)\n",
    "\n",
    "p_l1_low = _make_heatmap(data=sorted_l1_low, labels_dict=lables_l1_low, labels_dict_reversed=re_labels_l1_low,\n",
    "                       title=\"Esatimated inverse covariance (sparse + low-rank) sorted by l1-norm of the PCs\", width=width, height=height,\n",
    "                       label_size=label_size)\n",
    "show(p_l1_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 1500\n",
    "height = 1500\n",
    "label_size = \"8pt\"\n",
    "\n",
    "# for visualization reasons we transform inverse covaraince to negative inverse covaraince, i.e., multiply by -1\n",
    "sgl = -1 * pd.DataFrame(P_SGL.solution.precision_, columns=asv_names, index=asv_names)\n",
    "adapt = -1 * pd.DataFrame(P_SGL_adapt.solution.precision_, columns=vis_df.columns, index=vis_df.columns)\n",
    "low = -1 * pd.DataFrame(P_SGL_low.solution.precision_, columns=asv_names, index=asv_names)\n",
    "\n",
    "\n",
    "lables_sgl, re_labels_sgl = create_label_dict(sgl)\n",
    "lables_adapt, re_labels_adapt = create_label_dict(adapt)\n",
    "lables_low, re_labels_low = create_label_dict(low)\n",
    "\n",
    "p_sgl = _make_heatmap(data=sgl, labels_dict=lables_sgl, labels_dict_reversed=re_labels_sgl,\n",
    "                       title=\"SGL estimated (negative) inverse covariance\", width=width, height=height,\n",
    "                       label_size=label_size)\n",
    "\n",
    "p_adapt = _make_heatmap(data=adapt, labels_dict=lables_adapt, labels_dict_reversed=re_labels_adapt,\n",
    "                       title=\"Adaptive estimated (negative) inverse covariance\", width=width, height=height,\n",
    "                       label_size=label_size)\n",
    "\n",
    "p_low = _make_heatmap(data=low, labels_dict=lables_low, labels_dict_reversed=re_labels_low,\n",
    "                       title=\"SGL+low-rank estimated (negative) inverse covariance\", width=width, height=height,\n",
    "                       label_size=label_size)\n",
    "show(p_sgl)\n",
    "show(p_adapt)\n",
    "show(p_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_cols = list(adapt.iloc[:, -n_cov:].columns)\n",
    "\n",
    "asv69_edges = [\"ASV_18\", \"ASV_51\", \"ASV_46\", \"ASV_13\", \"ASV_7\", \"ASV_5\"]\n",
    "asv69_edges_adapt = meta_cols + [\"ASV_18\", \"ASV_51\", \"ASV_5\", \"ASV_46\"]\n",
    "asv69_edges_low = asv69_edges\n",
    "\n",
    "sgl_edges = sgl[sgl.columns.intersection(asv69_edges)].loc[asv69_edges]\n",
    "adapt_edges = adapt[adapt.columns.intersection(asv69_edges_adapt)].loc[asv69_edges_adapt]\n",
    "low_edges = low[low.columns.intersection(asv69_edges_low)].loc[asv69_edges_low]\n",
    "\n",
    "\n",
    "G_SGL = create_graph(sgl_edges, threshold=0.01)\n",
    "G_adapt = create_graph(adapt_edges, threshold=0.01)\n",
    "G_low = create_graph(low_edges, threshold=0.01)\n",
    "\n",
    "\n",
    "width, height= 1000, 1000\n",
    "\n",
    "network_sgl = plot_network(G_SGL, title=\"SGL\", height=height, width=width)\n",
    "network_adapt = plot_network(G_adapt, title=\"Adaptive\",  height=height, width=width)\n",
    "network_low = plot_network(G_low, title=\"Low-rank\",  height=height, width=width)\n",
    "\n",
    "show(network_sgl)\n",
    "show(network_adapt)\n",
    "show(network_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

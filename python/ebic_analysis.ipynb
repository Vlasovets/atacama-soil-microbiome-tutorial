{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import bisect\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "from itertools import chain\n",
    "from math import pi\n",
    "from sklearn import preprocessing\n",
    "from GGLasso.gglasso.problem import glasso_problem\n",
    "from GGLasso.gglasso.helper.model_selection import ebic\n",
    "from utils import transform_features, scale_array_by_diagonal\n",
    "from utils import PCA\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from networkx.utils import cuthill_mckee_ordering\n",
    "\n",
    "from bokeh.io import output_notebook, show, save\n",
    "from bokeh.models import Range1d, Circle, ColumnDataSource, MultiLine, HoverTool, LabelSet, PointDrawTool\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.plotting import from_networkx\n",
    "from bokeh.palettes import RdBu, Blues8\n",
    "from bokeh.models import HoverTool, Panel, Tabs, ColorBar, LinearColorMapper\n",
    "from bokeh.layouts import row\n",
    "\n",
    "# from latentcor import gen_data, get_tps, latentcor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(X, L, inverse=True):\n",
    "    sig, V = np.linalg.eigh(L)\n",
    "\n",
    "    # sort eigenvalues in descending order\n",
    "    sig = sig[::-1]\n",
    "    V = V[:, ::-1]\n",
    "\n",
    "    ind = np.argwhere(sig > 1e-9)\n",
    "\n",
    "    if inverse:\n",
    "        loadings = V[:, ind] @ np.diag(np.sqrt(1 / sig[ind]))\n",
    "    else:\n",
    "        loadings = V[:, ind] @ np.diag(np.sqrt(sig[ind]))\n",
    "\n",
    "    # compute the projection\n",
    "    zu = X.values @ loadings\n",
    "\n",
    "    return zu, loadings, np.round(sig[ind].squeeze(), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotly_heatmap(z, x, y, title: str, x_label: str, y_label: str, zmin: int, zmax: int,\n",
    "            height: int=1200, width: int=1200, colorscale: str='RdBu_r'):\n",
    "    # Create a Plotly heatmap using the correlation matrix\n",
    "    heatmap = go.Heatmap(z=z, x=x, y=y, colorscale=colorscale, zmin = zmin, zmax = zmax)\n",
    "    # Create a layout for the heatmap\n",
    "    layout = go.Layout(title=title, xaxis=dict(title=x_label), yaxis=dict(title=y_label), \n",
    "                       height=height, width=width, xaxis_tickangle=45)\n",
    "    \n",
    "    # Create a figure object and add the heatmap to it\n",
    "    fig = go.Figure(data=[heatmap], layout=layout)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_heatmap(data: pd.DataFrame(), title: str = None, labels_dict: dict = None,\n",
    "                  labels_dict_reversed: dict = None,\n",
    "                  width: int = 1500, height: int = 1500, label_size: str = \"5pt\",\n",
    "                  title_size: str = \"24pt\", not_low_rank: bool = True):\n",
    "    nlabels = len(labels_dict)\n",
    "    shifted_labels_dict = {k + 0.5: v for k, v in labels_dict.items()}\n",
    "    shifted_labels_dict_reversed = {k + 0.5: v for k, v in labels_dict_reversed.items()}\n",
    "\n",
    "    df = data.iloc[::-1]  # rotate matrix 90 degrees\n",
    "    df = pd.DataFrame(df.stack(), columns=['covariance']).reset_index()\n",
    "    df.columns = [\"taxa_y\", \"taxa_x\", \"covariance\"]\n",
    "    df = df.replace({\"taxa_x\": labels_dict, \"taxa_y\": labels_dict})\n",
    "\n",
    "    color_list, colors = _get_colors(df=df)\n",
    "    # min_value = df['covariance'].min()\n",
    "    # max_value = df['covariance'].max()\n",
    "    # mapper = LinearColorMapper(palette=colors, low=min_value, high=max_value)\n",
    "    mapper = LinearColorMapper(palette=colors, low=-1, high=1)\n",
    "    color_bar = ColorBar(color_mapper=mapper, location=(0, 0))\n",
    "\n",
    "    bottom, top, left, right = _get_bounds(nlabels=nlabels)\n",
    "\n",
    "    source = ColumnDataSource(\n",
    "        dict(top=top, bottom=bottom, left=left, right=right, color_list=color_list,\n",
    "             taxa_x=df['taxa_x'], taxa_y=df['taxa_y'], covariance=df['covariance']))\n",
    "\n",
    "    bokeh_tools = [\"save, zoom_in, zoom_out, wheel_zoom, box_zoom, crosshair, reset, hover\"]\n",
    "\n",
    "    p = figure(plot_width=width, plot_height=height, x_range=(0, nlabels), y_range=(0, nlabels),\n",
    "               title=title, title_location='above', x_axis_location=\"below\",\n",
    "               tools=bokeh_tools, toolbar_location='left')\n",
    "\n",
    "    p.quad(top=\"top\", bottom=\"bottom\", left=\"left\", right=\"right\", line_color='white',\n",
    "           color=\"color_list\", source=source)\n",
    "    p.xaxis.major_label_orientation = pi / 4\n",
    "    p.yaxis.major_label_orientation = \"horizontal\"\n",
    "    p.xaxis.major_label_text_font_size = label_size\n",
    "    p.yaxis.major_label_text_font_size = label_size\n",
    "    p.title.text_font_size = title_size\n",
    "    p.add_layout(color_bar, 'right')\n",
    "    p.toolbar.autohide = True\n",
    "    p.xaxis.ticker = [x + 0.5 for x in\n",
    "                      list(range(0, nlabels))]  ### shift label position to the center\n",
    "    p.yaxis.ticker = [x + 0.5 for x in list(range(0, nlabels))]\n",
    "    p.xaxis.major_label_overrides = shifted_labels_dict\n",
    "    p.yaxis.major_label_overrides = shifted_labels_dict_reversed\n",
    "\n",
    "    hover = p.select(dict(type=HoverTool))\n",
    "    hover.tooltips = [\n",
    "        (\"taxa_x\", \"@taxa_x\"),\n",
    "        (\"taxa_y\", \"@taxa_y\"),\n",
    "        (\"covariance\", \"@covariance\"),\n",
    "    ]\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_network(G, title, width, height, node_size=None, amplify_x=10):\n",
    "    #Establish which categories will appear when hovering over each node\n",
    "    HOVER_TOOLTIPS = [(\"Character\", \"@index\")]\n",
    "    hover = HoverTool(tooltips=[('','@index')])\n",
    "    tools = [\"save, zoom_in, zoom_out, wheel_zoom, box_zoom, crosshair, reset, hover, pan\"]\n",
    "\n",
    "    #Create a plot â€” set dimensions, toolbar, and title\n",
    "    plot = figure(tooltips = HOVER_TOOLTIPS, plot_width=width, plot_height=height,\n",
    "                  tools=tools, active_scroll='wheel_zoom',\n",
    "                x_range=Range1d(-10.1, 10.1), \n",
    "                  y_range=Range1d(-10.1, 10.1), title=title)\n",
    "    \n",
    "    \n",
    "    \n",
    "    color_map = [\"#88CCEE\" if \"ASV\" in j else \"#DDCC77\" for j in G.nodes()] \n",
    "    nx.set_node_attributes(G, {j: {'color': color_map[i]} for i, j in enumerate(G.nodes())})\n",
    "\n",
    "    if node_size is not None:\n",
    "        n_degrees = {k: 15*v for k,v in G.degree()} \n",
    "        nx.set_node_attributes(G, n_degrees, 'node_size')\n",
    "        node_size = 'node_size'\n",
    "    else:\n",
    "        node_size = 40\n",
    "\n",
    "    network_graph = from_networkx(G, nx.spring_layout, scale=10, center=(0, 0))\n",
    "\n",
    "\n",
    "    #Set node size and color\n",
    "    network_graph.node_renderer.glyph = Circle(size=node_size,  fill_color=\"color\")\n",
    "    \n",
    "    #Set edge width and color green - positive, red - negative\n",
    "    network_graph.edge_renderer.data_source.data[\"line_width\"] = [G.get_edge_data(a,b)['covariance']*amplify_x for a, b in G.edges()]  ### amplify edges strengh\n",
    "    network_graph.edge_renderer.data_source.data[\"line_color\"] = [\"#117733\" if G.get_edge_data(a, b)['covariance'] >= 0 else \"#CC6677\" for a, b in G.edges()]\n",
    "    network_graph.edge_renderer.glyph.line_width = {'field': 'line_width'} \n",
    "    network_graph.edge_renderer.glyph.line_color = {'field': 'line_color'}\n",
    "\n",
    "    #Add network graph to the plot\n",
    "    plot.renderers.append(network_graph)\n",
    "    \n",
    "    x, y = zip(*network_graph.layout_provider.graph_layout.values())\n",
    "    node_labels = list(G.nodes)\n",
    "    source = ColumnDataSource({'x': x, 'y': y, 'asv': [node_labels[i] for i in range(len(x))]})\n",
    "    labels = LabelSet(x='x', y='y', text='asv', x_offset=30, y_offset=-15, source=source, render_mode='canvas', text_font_size='12pt')\n",
    "\n",
    "    plot.renderers.append(labels)    \n",
    "\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_corr(corr_array, inplace=False):\n",
    "    \"\"\"\n",
    "    Rearranges the correlation matrix, corr_array, so that groups of highly \n",
    "    correlated variables are next to eachother \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    corr_array : pandas.DataFrame or numpy.ndarray\n",
    "        a NxN correlation matrix \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame or numpy.ndarray\n",
    "        a NxN correlation matrix with the columns and rows rearranged\n",
    "    \"\"\"\n",
    "    pairwise_distances = sch.distance.pdist(corr_array)\n",
    "    linkage = sch.linkage(pairwise_distances, method='complete')\n",
    "    cluster_distance_threshold = pairwise_distances.max()/2\n",
    "    idx_to_cluster_array = sch.fcluster(linkage, cluster_distance_threshold, \n",
    "                                        criterion='distance')\n",
    "    idx = np.argsort(idx_to_cluster_array)\n",
    "    \n",
    "    if not inplace:\n",
    "        corr_array = corr_array.copy()\n",
    "    \n",
    "    if isinstance(corr_array, pd.DataFrame):\n",
    "        return corr_array.iloc[idx, :].T.iloc[idx, :]\n",
    "    return corr_array[idx, :][:, idx]\n",
    "\n",
    "\n",
    "# fig = px.imshow(-1*cluster_corr(precision_SGL), color_continuous_scale='RdBu_r', zmin=-1, zmax=1)\n",
    "# fig.update_layout(margin = dict(t=100,r=100,b=100,l=100), width = 1000, height = 1000,\n",
    "#                  title='Clustered Estimated inverse covariance: ASVs', title_x=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(corr_matrix: pd.DataFrame(), threshold: float):\n",
    "    #take the upper part only\n",
    "    upper = np.triu(np.ones(corr_matrix.shape)).astype(bool)\n",
    "    df = corr_matrix.where(upper)\n",
    "    df = pd.DataFrame(corr_matrix.stack(), columns=['covariance']).reset_index()\n",
    "    df.columns = [\"source\", \"target\", \"covariance\"]\n",
    "    \n",
    "    #remove diagonal entries\n",
    "    #df = df[df['covariance'] <= threshold]\n",
    "    df = df[abs(df['covariance']) >= threshold]\n",
    "    #remove diagonal entries\n",
    "    df = df[df['source'] != df['target']]\n",
    "    #remove zero entries\n",
    "    df = df[df['covariance'] != 0]\n",
    "    \n",
    "    #build graph\n",
    "    G = nx.from_pandas_edgelist(df, edge_attr=\"covariance\")\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_covariates(counts=pd.DataFrame(), metadata=pd.DataFrame(), L=np.ndarray, y=str, PC=0):\n",
    "    proj, loadings, eigv = PCA(counts.dropna(), L, inverse=True)\n",
    "    r = np.linalg.matrix_rank(L)\n",
    "    eigv_sum = np.sum(eigv)\n",
    "    var_exp = [(value / eigv_sum) for value in sorted(eigv, reverse=True)]\n",
    "    \n",
    "    depth = pd.DataFrame(data=raw.sum(axis=0), columns=[\"sequencing depth\"])\n",
    "    metadata = depth.join(metadata)\n",
    "    \n",
    "    pc_columns = list('PC{0} ({1}%)'.format(i+1, str(100 * var_exp[i])[:4]) for i in range(0, r))\n",
    "    df_proj = pd.DataFrame(proj, columns=pc_columns, index=counts.index)\n",
    "    df = df_proj.join(metadata)\n",
    "    \n",
    "    varName1 = 'PC{0} ({1}%)'.format(PC+1, str(100 * var_exp[PC])[:4])\n",
    "    varName2 = y\n",
    "    # varName2 = 'PC{0} ({1}%)'.format(PC+2, str(100 * var_exp[1])[:4])\n",
    "    df['x'] = df[varName1]\n",
    "    df['y'] = df[varName2]\n",
    "\n",
    "    source = ColumnDataSource(df)\n",
    "\n",
    "    p0 = figure(tools='save, zoom_in, zoom_out, wheel_zoom, box_zoom, reset', plot_width=800, plot_height=800,\n",
    "                active_scroll=\"wheel_zoom\",\n",
    "                x_axis_label=varName1, y_axis_label=varName2,\n",
    "                tooltips=[(varName1, \"@\" + varName1),\n",
    "                          (varName2, \"@\" + varName2)\n",
    "                          ],\n",
    "                title=varName1 + \" vs \" + varName2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    rdbu = plt.get_cmap('Blues_r')\n",
    "    cmap = ListedColormap(rdbu(np.arange(256)))\n",
    "    # Create a list of hex color codes from the colormap\n",
    "    colors = [cmap(i)[:3] for i in range(256)]\n",
    "    colors = ['#' + ''.join([format(int(c * 255), '02x') for c in color]) for color in colors]\n",
    "    colors = colors[::-1]  # red - positive, blue - negative\n",
    "    exp_cmap = LinearColorMapper(palette=colors, low=depth.values.min(), high=depth.values.max())\n",
    "    \n",
    "    #exp_cmap = LinearColorMapper(palette=Blues8[::-1], low=min(df['sequencing depth'].values), high=max(df['sequencing depth'].values))\n",
    "    p0.circle('x', 'y', source=source, size=15, line_color=None, fill_color={\"field\": \"sequencing depth\", \"transform\": exp_cmap}, fill_alpha=0.3)\n",
    "\n",
    "    color_bar_plot = figure(title='sequencing depth', title_location=\"right\",\n",
    "                            height=500, width=150, toolbar_location=None, min_border=0,\n",
    "                            outline_line_color=None)\n",
    "\n",
    "    bar = ColorBar(color_mapper=exp_cmap, location=(1, 1))\n",
    "    #bar = ColorBar(color_mapper=exp_cmap, location=(1, 1))\n",
    "\n",
    "    color_bar_plot.add_layout(bar, 'right')\n",
    "    color_bar_plot.title.align = \"center\"\n",
    "    color_bar_plot.title.text_font_size = '12pt'\n",
    "\n",
    "    layout = row(p0, color_bar_plot)\n",
    "\n",
    "    return layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels(df):\n",
    "    i = 1\n",
    "    for col in df.columns:\n",
    "        # length of ASVs identifier\n",
    "        if len(col) == 32:\n",
    "            asv_name = \"ASV_{0}\".format(i)\n",
    "            id_dict[asv_name] = col\n",
    "            df.rename(columns={col: asv_name}, inplace=True)\n",
    "\n",
    "            i += 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_bounds(nlabels: int):\n",
    "    bottom = list(chain.from_iterable([[ii] * nlabels for ii in range(nlabels)]))\n",
    "    top = list(chain.from_iterable([[ii + 1] * nlabels for ii in range(nlabels)]))\n",
    "    left = list(chain.from_iterable([list(range(nlabels)) for ii in range(nlabels)]))\n",
    "    right = list(chain.from_iterable([list(range(1, nlabels + 1)) for ii in range(nlabels)]))\n",
    "\n",
    "    return bottom, top, left, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_colors(df: pd.DataFrame()):\n",
    "    rdbu = plt.get_cmap('RdBu')\n",
    "    cmap = ListedColormap(rdbu(np.arange(256)))\n",
    "    \n",
    "    # Create a list of hex color codes from the colormap\n",
    "    colors = [cmap(i)[:3] for i in range(256)]\n",
    "    colors = ['#' + ''.join([format(int(c * 255), '02x') for c in color]) for color in colors]\n",
    "    colors = colors[::-1]  # red - positive, blue - negative\n",
    "\n",
    "    ccorr = np.arange(-1, 1, 1 / (len(colors) / 2))\n",
    "    color_list = []\n",
    "    for value in df.covariance.values:\n",
    "        ind = bisect.bisect_left(ccorr, value)  # smart array insertion\n",
    "        if ind == 0:  # avoid ind == -1 on the next step\n",
    "            ind = ind + 1\n",
    "        color_list.append(colors[ind - 1])\n",
    "    return color_list, colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_dict(df):\n",
    "    n_labels = len(df.columns)\n",
    "    labels_dict = dict(zip(range(n_labels), df.columns))\n",
    "    labels_dict_reversed = dict(zip(range(n_labels),list(labels_dict.values())[::-1]))\n",
    "    \n",
    "    return labels_dict, labels_dict_reversed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scater_plot(x, y, width=800, height=600, size=3):\n",
    "    bokeh_tools = [\"save, zoom_in, zoom_out, wheel_zoom, box_zoom, crosshair, reset, hover\"]\n",
    "    p = figure(plot_width=width, plot_height=height, tools=bokeh_tools, toolbar_location='left')\n",
    "\n",
    "    source = ColumnDataSource({'x': x, 'y': y})\n",
    "\n",
    "    p.circle(\"x\", \"y\", size=3*size, source=source, line_color=None)\n",
    "\n",
    "    p.xaxis.axis_label = x.name\n",
    "    p.yaxis.axis_label = y.name\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#count table\n",
    "# raw = pd.read_csv('data/composition_feature-table.tsv', sep='\\t', index_col = 0)\n",
    "\n",
    "raw = pd.read_csv(\"data/atacama_counts.tsv\", sep='\\t', index_col = 0)\n",
    "\n",
    "print(\"Some columns contain only zeros:\", (raw == 0).all().any())\n",
    "\n",
    "raw_T = raw.T \n",
    "\n",
    "### zero-inflation per ASV across all samples\n",
    "zero_perc = (raw_T == 0).mean()\n",
    "\n",
    "# mask = zero_perc > 0.8\n",
    "\n",
    "mask = zero_perc > 0.9\n",
    "\n",
    "raw_filt_T = raw_T.drop(columns=zero_perc[mask].index)\n",
    "\n",
    "raw_filt = raw_filt_T.T\n",
    "\n",
    "print(\"Some columns contain only zeros:\", (raw_filt == 0).all().any())\n",
    "\n",
    "zero_cols = [col for col in raw_filt.columns if all(raw_filt[col] == 0)]\n",
    "\n",
    "raw_filt_final = raw_filt.drop(zero_cols, axis=1)\n",
    "\n",
    "print(\"Some columns contain only zeros:\", (raw_filt_final == 0).all().any())\n",
    "\n",
    "raw_filt_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "taxa = pd.read_csv(\"data/taxonomy.tsv\", sep=\"\\t\", index_col=0)\n",
    "\n",
    "taxonomy_levels = {\"domain\": '^d__', \n",
    "                   \"phylum\": '^p__', \n",
    "                   \"class\": '^c__', \n",
    "                   \"order\": '^o__', \n",
    "                   \"family\": '^f__',\n",
    "                   \"genus\": '^g__', \n",
    "                   \"species\": '^s__'\n",
    "                  }\n",
    "\n",
    "# split taxonomic ranks in different columns\n",
    "taxa_sep = taxa['Taxon'].str.split(';', expand=True)\n",
    "\n",
    "#rename taxonomic ranks with full names\n",
    "taxa_sep.columns = taxonomy_levels.keys()\n",
    "\n",
    "# drop missing species\n",
    "taxa_sep = taxa_sep[taxa_sep.species.notnull()]\n",
    "\n",
    "# remove blank spaces from taxonomic ranks\n",
    "taxa_sep[taxa_sep.columns] = taxa_sep.apply(lambda x: x.str.strip())\n",
    "\n",
    "taxa_sep.shape\n",
    "\n",
    "# substract \"s\" from the string names\n",
    "taxa_sep['species'] = taxa_sep['species'].map(lambda x: x.lstrip('s'))\n",
    "taxa_sep['species'] = taxa_sep['genus'] + taxa_sep['species']\n",
    "\n",
    "species = taxa_sep['species'].to_dict()\n",
    "\n",
    "species_names = dict(taxa_sep['genus'])\n",
    "\n",
    "# test = df.copy()\n",
    "\n",
    "# test = test.rename(columns=species_names)\n",
    "# test.columns\n",
    "\n",
    "# new_col_names = {col: f\"ASV_{i}\" for i, col in enumerate(test.columns) if len(col) == 32}\n",
    "\n",
    "# # Use the `rename()` method to change the column names using the dictionary\n",
    "# test = test.rename(columns=new_col_names)\n",
    "\n",
    "# test.to_csv(\"data/asv_covariates.csv\")\n",
    "\n",
    "# taxa_dict = dict()\n",
    "\n",
    "# for level in taxa_sep.columns:\n",
    "#     df_level = raw.join(taxa_sep[level])\n",
    "#     df_level = df_level.groupby(level).sum()\n",
    "#     taxa_dict[level] = df_level\n",
    "    \n",
    "# taxa_dict[\"ASV\"] = raw.copy()\n",
    "    \n",
    "# taxa_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_filt_final.index = raw_filt_final.index.map(species)\n",
    "raw_filt_final.index = raw_filt_final.index.fillna('unknown species')\n",
    "\n",
    "# Check and update index values with length less than 5\n",
    "new_index = []\n",
    "i = 1\n",
    "for idx in raw_filt_final.index:\n",
    "    if idx == \"unknown species\":\n",
    "        new_idx = f\"unknown species {i}\"\n",
    "        i += 1\n",
    "    else:\n",
    "        new_idx = idx\n",
    "    new_index.append(new_idx)\n",
    "raw_filt_final.index = new_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_filt_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorscale = [[0, 'rgb(255, 255, 255)'], [1, 'rgb(105, 0, 95)']]\n",
    "\n",
    "fig_raw = plotly_heatmap(z=raw_filt_final, x=raw_filt_final.columns, y=raw_filt_final.index, zmin = 0, zmax = 1,\n",
    "                   title='Data Heatmap', x_label='Samples', y_label ='Taxa', width=1400, height=900, colorscale= colorscale)\n",
    "\n",
    "fig_raw.show()\n",
    "\n",
    "# fig_raw.write_image(\"plots/raw_heatmap.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(raw_filt_final.T == 0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 8))\n",
    "\n",
    "sns.heatmap(raw_filt_final, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_dict = dict()\n",
    "\n",
    "# for key, item in taxa_dict.items():\n",
    "#     print(\"\\n\", key)\n",
    "    \n",
    "#     df = taxa_dict[key]\n",
    "#     if (df == 0).all().any():\n",
    "        \n",
    "#         zero_cols = df.columns[(df == 0).all()]\n",
    "#         print(\"{0} samples are dropped since there is no variance\". format(list(zero_cols)))\n",
    "\n",
    "#         df = df.drop(zero_cols, axis=1)\n",
    "#         print(\"Shape BEFORE aggregation:\", item.shape)\n",
    "#         print(\"Shape AFTER aggregation:\", df.shape)\n",
    "#     print(\"Some columns contain only zeros:\", (df == 0).all().any())\n",
    "    \n",
    "#     # # # # calculate percentage of zeros for each row\n",
    "#     percentage_zeros = (df == 0).sum(axis=1) / len(df.columns) * 100\n",
    "        \n",
    "#     # drop rows with more than 80% of zeros\n",
    "#     df_filt = df[percentage_zeros <= 80]\n",
    "#     if (df_filt == 0).all().any():\n",
    "        \n",
    "#         zero_cols = df_filt.columns[(df_filt == 0).all()]\n",
    "#         print(\"{0} samples are dropped since there is no variance\". format(list(zero_cols)))\n",
    "\n",
    "#         df_filt = df_filt.drop(zero_cols, axis=1)\n",
    "#         print(\"Shape BEFORE filtering:\", df.shape)\n",
    "#         print(\"Shape AFTER filtering:\", df_filt.shape)\n",
    "#     print(\"Some columns contain only zeros:\", (df_filt == 0).all().any())\n",
    "        \n",
    "#     count_dict[key] = df_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, item in count_dict.items():\n",
    "#     if item.shape[0] > 1:\n",
    "#         # Create the scatter matrix\n",
    "#         fig = ff.create_scatterplotmatrix(\n",
    "#             item.T, \n",
    "#             diag='histogram',\n",
    "#             height=1600, width=1600,\n",
    "#             title=key\n",
    "#         )\n",
    "\n",
    "#         # Show the plot\n",
    "#         fig.write_image(\"plots/{}_scatter.png\".format(key))\n",
    "# fig = ff.create_scatterplotmatrix(\n",
    "#             raw_filt_final.T, \n",
    "#             diag='histogram',\n",
    "#             height=1600, width=1600,\n",
    "#             title=key\n",
    "#         )\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### clr-transformation\n",
    "# clr = transform_features(raw, transformation=\"mclr\")\n",
    "# clr = transform_features(phylum, transformation=\"mclr\")\n",
    "# clr = transform_features(phylum_filt, transformation=\"mclr\")\n",
    "# clr = transform_features(count_dict['ASV'], transformation=\"mclr\")\n",
    "\n",
    "clr = transform_features(raw_filt_final, transformation=\"mclr\")\n",
    "# clr.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colorscale = [[0, 'rgb(255, 255, 255)'], [1, 'rgb(20, 83, 145)']]\n",
    "\n",
    "# fig_clr = plotly_heatmap(z=clr, x=clr.columns, y=clr.index, zmin = 0, zmax = 1,\n",
    "#                    title='Data Heatmap', x_label='Samples', y_label ='Taxa', width=1400, height=900, colorscale= colorscale)\n",
    "\n",
    "# fig_clr.show()\n",
    "\n",
    "# # fig_raw.write_image(\"plots/raw_heatmap.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as pyo\n",
    "\n",
    "def heatmap(z, x, y, title: str, x_label: str, y_label: str, zmin: int, zmax: int,\n",
    "            height: int=1200, width: int=1200):\n",
    "    # Create a Plotly heatmap using the correlation matrix\n",
    "    heatmap = go.Heatmap(z=z, x=x, y=y, colorscale='RdBu_r', zmin = zmin, zmax = zmax)\n",
    "    # Create a layout for the heatmap\n",
    "    layout = go.Layout(title=title, xaxis=dict(title=x_label), yaxis=dict(title=y_label), \n",
    "                       height=height, width=width)\n",
    "    # Create a figure object and add the heatmap to it\n",
    "    fig = go.Figure(data=[heatmap], layout=layout)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clr.values.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_X = heatmap(z=clr.T, x=clr.index, y=clr.columns, zmin = clr.values.min(), zmax = clr.values.max(),\n",
    "                title='mclr-transformed data', x_label='Samples', y_label ='Taxa')\n",
    "\n",
    "# Display the heatmap\n",
    "pyo.iplot(fig_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 8))\n",
    "\n",
    "sns.heatmap(clr, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv('data/acm_meta.tsv', sep='\\t', index_col = 0)\n",
    "\n",
    "# select only numeric features\n",
    "meta = meta.loc[:, meta.iloc[0, :] != 'categorical']\n",
    "meta = meta.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "### select most interesting columns\n",
    "selected_columns = ['ph', 'toc', 'ec', 'average-soil-relative-humidity', 'average-soil-temperature']\n",
    "\n",
    "meta = meta[selected_columns]\n",
    "\n",
    "# drop QIIME2 header\n",
    "meta = meta.iloc[1:]\n",
    "# fill missing values with zeros\n",
    "meta = meta.fillna(0)\n",
    "\n",
    "#scale data\n",
    "scaler = preprocessing.StandardScaler().fit(meta)\n",
    "meta_scaled = scaler.transform(meta)\n",
    "meta_scaled = pd.DataFrame(meta_scaled, index=meta.index, columns=meta.columns)\n",
    "# meta_scaled.to_csv(\"data/meta_scaled.csv\")\n",
    "\n",
    "# transpose count data\n",
    "clr_T = clr.T\n",
    "# join by sample id\n",
    "df = clr_T.join(meta_scaled)\n",
    "# df.to_csv(\"data/asv_meta.csv\", index=True)\n",
    "\n",
    "# Rename long feature IDs with concise names\n",
    "vis_df = df.copy()\n",
    "# id_dict = dict()\n",
    "# vis_df = add_labels(vis_df)\n",
    "\n",
    "#calculate covariance\n",
    "n_cov = meta_scaled.shape[1]\n",
    "asv = df.iloc[:, :-n_cov]\n",
    "# asv.to_csv(\"data/asv.csv\", index=True)\n",
    "\n",
    "### latent corr\n",
    "# clean_types_asv = get_tps(asv)\n",
    "# print(clean_types_asv)\n",
    "# lat_cor_asv = latentcor(asv, tps = clean_types_asv, method ='original', use_nearPD=False)\n",
    "# with open('data/lat_corr.npy', 'rb') as f:\n",
    "#     corr = np.load(f)\n",
    "\n",
    "S = np.cov(asv.T.values, bias=True)\n",
    "\n",
    "# # # correlation between ASVs ONLY\n",
    "corr = scale_array_by_diagonal(S)\n",
    "# corr = lat_cor_asv[\"R\"].values\n",
    "\n",
    "# # #add labels\n",
    "asv_names = vis_df.iloc[:, :-n_cov].columns\n",
    "vis_S = pd.DataFrame(corr, columns=asv_names, index=asv_names)\n",
    "\n",
    "\n",
    "# clean_types_all = get_tps(df)\n",
    "# print(clean_types_all)\n",
    "# lat_cor_all = latentcor(df, tps = clean_types_all, method ='original', use_nearPD=False)\n",
    "\n",
    "# corr_meta = lat_cor_all[\"R\"].values\n",
    "# with open('data/lat_corr_meta.npy', 'rb') as f:\n",
    "#     corr_meta = np.load(f)\n",
    "# vis_S_meta = pd.DataFrame(corr_meta, columns=vis_df.columns, index=vis_df.columns)\n",
    "\n",
    "# # # # # correlation between ASVs and covariates\n",
    "S_meta = np.cov(df.T.values, bias=True)\n",
    "corr_meta = scale_array_by_diagonal(S_meta)\n",
    "vis_S_meta = pd.DataFrame(corr_meta, columns=vis_df.columns, index=vis_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_values = scaler.mean_\n",
    "std_values = scaler.scale_\n",
    "\n",
    "std_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(5,1,figsize=(5, 12))\n",
    "# meta.hist(ax=axis,  color='#5C1360')\n",
    "meta_scaled.hist(ax=axis,  color='#5C1360')\n",
    "\n",
    "# fig.savefig('plots/meta_unscaled.png')\n",
    "# fig.savefig('plots/meta_scaled.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clr.iloc[:, 1].plot.hist(bins=10, alpha=1, color='#5C1360').get_figure().savefig('plots/mclr_count.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_filt_final.iloc[:, 1].plot.hist(bins=10, alpha=1, color='#5C1360').get_figure().savefig('plots/raw_count.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "sns.heatmap(corr, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "sns.heatmap(corr_meta, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 1500\n",
    "height = 1500\n",
    "label_size = \"36pt\"\n",
    "lables_0, re_labels_0 = create_label_dict(vis_S)\n",
    "\n",
    "p0 = _make_heatmap(data=vis_S, labels_dict=lables_0, labels_dict_reversed=re_labels_0,\n",
    "                       title=\"Correlation: ASVs\", width=width, height=height,\n",
    "                       label_size=label_size)\n",
    "\n",
    "meta_corr = vis_S_meta.iloc[-n_cov:, -n_cov:]\n",
    "lables_1, re_labels_1 = create_label_dict(meta_corr)\n",
    "\n",
    "p1 = _make_heatmap(data=meta_corr, labels_dict=lables_1, labels_dict_reversed=re_labels_1,\n",
    "                       title=\"Correlation: covariates\", width=width, height=height,\n",
    "                       label_size=label_size)\n",
    "\n",
    "# drop highly correlated covariates\n",
    "# hcorr_cov = ['relative-humidity-soil-high', 'relative-humidity-soil-low', 'percent-relative-humidity-soil-100', 'temperature-soil-high', 'temperature-soil-low']\n",
    "# hcorr_cov = ['relative-humidity-soil-low', 'percent-relative-humidity-soil-100', 'temperature-soil-high', 'temperature-soil-low']\n",
    "\n",
    "\n",
    "# for frame in [vis_S_meta, df, vis_df]:\n",
    "#     frame.drop(hcorr_cov, axis=1, inplace=True)\n",
    "#     frame.rename(columns={'average-soil-relative-humidity':'average humidity','average-soil-temperature': 'average temperature',}, inplace=True)\n",
    "\n",
    "vis_S_meta = vis_S_meta.T\n",
    "# vis_S_meta.drop(hcorr_cov, axis=1, inplace=True)\n",
    "# vis_S_meta.rename(columns={'average-soil-relative-humidity':'average humidity','average-soil-temperature': 'average temperature',}, inplace=True)\n",
    "\n",
    "n_cov = df.shape[1] - asv.shape[1]\n",
    "lables_2, re_labels_2 = create_label_dict(vis_S_meta)\n",
    "\n",
    "p2 = _make_heatmap(data=vis_S_meta, labels_dict=lables_2, labels_dict_reversed=re_labels_2,\n",
    "                       title=\"Correlation: ASVs + covariates\", width=width, height=height,\n",
    "                       label_size=label_size)\n",
    "\n",
    "# show(p0)\n",
    "# show(p1)\n",
    "# show(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g_vis_S = sns.clustermap(vis_S, method='average', cmap='RdBu', center=0, dendrogram_ratio=0.2, robust=True, cbar_pos=None)\n",
    "\n",
    "# get the order of the rows and columns\n",
    "row_order_vis_S = g_vis_S.dendrogram_row.reordered_ind\n",
    "col_order_vis_S = g_vis_S.dendrogram_col.reordered_ind\n",
    "\n",
    "vis_S_clust = vis_S.iloc[row_order_vis_S, col_order_vis_S]\n",
    "\n",
    "lables_vis_S_clust, re_labels_vis_S_clust = create_label_dict(vis_S_clust)\n",
    "\n",
    "p_vis_S_clust = _make_heatmap(data=vis_S_clust, labels_dict=lables_vis_S_clust, labels_dict_reversed=re_labels_vis_S_clust, # multiply by 3 for making edge visible on the heatmao\n",
    "                       title=\"Clustered correlation\", width=width, height=height,\n",
    "                       label_size=label_size)\n",
    "\n",
    "show(p_vis_S_clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_after_underscore(dct):\n",
    "    for key in dct:\n",
    "        if '_' in dct[key]:\n",
    "            dct[key] = dct[key].split('_')[0]\n",
    "    return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 1500\n",
    "height = 1500\n",
    "label_size = \"24pt\"\n",
    "\n",
    "latcorr = pd.read_csv(\"data/showcase_latent_corr.csv\", index_col=0)\n",
    "\n",
    "lables_3, re_labels_3 = create_label_dict(latcorr)\n",
    "\n",
    "lables_3 = remove_after_underscore(lables_3)\n",
    "re_labels_3 = remove_after_underscore(re_labels_3)\n",
    "\n",
    "p3 = _make_heatmap(data=latcorr, labels_dict=lables_3, labels_dict_reversed=re_labels_3,\n",
    "                       title=\"Correlation: ASVs + covariates\", width=width, height=height,\n",
    "                       label_size=label_size)\n",
    "show(p3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = asv.shape[0]\n",
    "p = asv.shape[1]\n",
    "print(\"Shape of data without covariates: {0}, {1}\".format(N, p))\n",
    "\n",
    "N_meta = df.shape[0]\n",
    "p_meta = df.shape[1]\n",
    "print(\"Shape of data with covariates: {0}, {1}\".format(N_meta, p_meta))\n",
    "\n",
    "#hyperparameters\n",
    "# lambda1_range = np.logspace(0, -4, 15)\n",
    "# mu1_range = np.logspace(0.9, 0.4, 10)\n",
    "\n",
    "lambda1_range = np.logspace(0, -3, 50)\n",
    "# mu1_range = np.logspace(-2, -2.5, 10)\n",
    "### for 0.9\n",
    "mu1_range = np.logspace(0, -2, 10)\n",
    "\n",
    "### for 0.8\n",
    "# mu1_range = np.logspace(-0.2, -0.5, 10)\n",
    "\n",
    "# lambda1_range = np.logspace(0, -1, 15)\n",
    "# mu1_range = np.logspace(-0.5, -4, 10)\n",
    "# lambda1_range = np.logspace(0, -2, 30)\n",
    "# mu1_range = np.logspace(-0.5, -2, 10)\n",
    "# lambda1_range = np.logspace(-1, -4, 30)\n",
    "# mu1_range = np.logspace(-0.5, -2, 10)\n",
    "modelselect_params = {'lambda1_range': lambda1_range, 'mu1_range': mu1_range}\n",
    "\n",
    "P_SGL = glasso_problem(corr, N, latent=False, do_scaling=False)\n",
    "P_SGL.model_selection(modelselect_params=modelselect_params, method='eBIC', gamma=0.01, store_all=True)\n",
    "\n",
    "P_SGL_low = glasso_problem(corr, N, latent=True, do_scaling=False)\n",
    "P_SGL_low.model_selection(modelselect_params=modelselect_params, method='eBIC', gamma=0.01, store_all=True)\n",
    "\n",
    "# create lambda matrix full of zeros\n",
    "shape_meta = (p_meta, p_meta)\n",
    "mask = np.zeros(shape_meta)\n",
    "# add small constant, so ADMM could converge\n",
    "mask = mask + 0.01\n",
    "# heavy penalize species\n",
    "n_bugs = len(asv.columns)\n",
    "bugs_block = np.ones((n_bugs, n_bugs))\n",
    "mask[0:n_bugs, 0:n_bugs] += bugs_block - 0.01\n",
    "lambda1_mask_exp = mask\n",
    "df_mask_exp = pd.DataFrame(lambda1_mask_exp, columns=vis_df.columns, index=vis_df.columns)\n",
    "\n",
    "modelselect_params[\"lambda1_mask\"] = lambda1_mask_exp\n",
    "P_SGL_adapt = glasso_problem(vis_S_meta.values, N_meta, latent=False, do_scaling=False)\n",
    "P_SGL_adapt.model_selection(modelselect_params=modelselect_params, method='eBIC', gamma=0.01, store_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SGL solution with lambda={lambda1} and mu={mu1}\".format(**P_SGL.reg_params))\n",
    "print(\"Adaptive SGL+low-rank solution with lambda={lambda1} and mu={mu1}\".format(**P_SGL_adapt.reg_params))\n",
    "print(\"SGL+low-rank solution with lambda={lambda1} and mu={mu1}\".format(**P_SGL_low.reg_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "P_SGL.modelselect_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ebic(S=corr, Theta=P_SGL.solution.precision_, N=N, gamma=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ebic(lambda1_range, P_SGL, P_SGL_low, P_SGL_adapt):\n",
    "    \"\"\"\n",
    "    Calculate eBIC for different solvers, lambda values, and gamma values.\n",
    "    \n",
    "    Args:\n",
    "        lambda1_range (list): List of lambda values.\n",
    "        P_SGL (glasso_problem obj): Object for P_SGL solver.\n",
    "        P_SGL_low (glasso_problem obj): Object for P_SGL_low solver.\n",
    "        P_SGL_adapt (glasso_problem obj): Object for P_SGL_adapt solver.\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing solver, lambda, gamma, and eBIC statistics.\n",
    "    \"\"\"\n",
    "    \n",
    "    P_SGL.label = \"sgl\"\n",
    "    P_SGL_low.label = \"low\"\n",
    "    P_SGL_adapt.label = \"adapt\"\n",
    "\n",
    "    stats_dict = { 'solver': [], 'lambda': [], 'gamma': [], 'eBIC': [], 'SP': []}\n",
    "\n",
    "    gamma_list = [0.001, 0.01, 0.1, 0.15, 0.25, 0.5, 0.7, 1]\n",
    "\n",
    "    for sol in [P_SGL, P_SGL_low, P_SGL_adapt]:\n",
    "        sigma = sol.solution.sample_covariance_\n",
    "        n = sol.solution.n_samples\n",
    "\n",
    "        for i in range(0, len(lambda1_range)):\n",
    "            theta = sol._all_theta[i, 0]\n",
    "            lambda1 = lambda1_range[i]\n",
    "            sparsity = sol.modelselect_stats[\"SP\"][i][0]\n",
    "\n",
    "            for gamma in gamma_list:\n",
    "                stats_dict['solver'].append(sol.label)\n",
    "                stats_dict['lambda'].append(lambda1)\n",
    "                stats_dict['gamma'].append(gamma)\n",
    "                stats_dict['eBIC'].append(ebic(S=sigma, Theta=theta, N=n, gamma=gamma))\n",
    "                stats_dict['SP'].append(sparsity)\n",
    "\n",
    "    stats_df = pd.DataFrame(stats_dict)\n",
    "    \n",
    "    return stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = calculate_ebic(lambda1_range, P_SGL, P_SGL_low, P_SGL_adapt)\n",
    "result_df[result_df[\"lambda\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l_list = []\n",
    "# g_list = []\n",
    "# ebic_list = []\n",
    "\n",
    "# for l1 in np.unique(test[\"lambda\"]):\n",
    "#     a = test[test['lambda'] == l1]\n",
    "    \n",
    "#     b = np.array(a[\"lambda\"])\n",
    "#     d = np.array(a[\"gamma\"])\n",
    "#     e = np.array(a[\"eBIC\"])\n",
    "    \n",
    "#     l_list.append(b)\n",
    "#     g_list.append(b)\n",
    "#     ebic_list.append(b)\n",
    "\n",
    "# l_list = np.array(l_list)\n",
    "# g_list = np.array(g_list)\n",
    "# ebic_list = np.array(ebic_list)\n",
    "# l_list.shape, g_list.shape, ebic_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for solver in result_df[\"solver\"].unique():\n",
    "    fig = px.scatter_3d(result_df[result_df[\"solver\"] == solver], x='lambda', y='gamma', z='eBIC')\n",
    "\n",
    "    # Set labels and title\n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis_title='log(lambda)',\n",
    "            yaxis_title='gamma',\n",
    "            zaxis_title='eBIC',\n",
    "            xaxis_type=\"log\"\n",
    "        ),\n",
    "        title=solver,\n",
    "        width=1000,  # Set the width of the plot (in pixels)\n",
    "        height=1000  # Set the height of the plot (in pixels)\n",
    "    )\n",
    "\n",
    "    # Show the plot\n",
    "    fig.write_html(\"plots/{0}_ebic_3D.html\".format(solver))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gamma_list = []\n",
    "lambda_list = []\n",
    "\n",
    "for l1 in test['lambda'].unique():\n",
    "    print(l1)\n",
    "    # Filter the DataFrame for lambda = 1\n",
    "    filtered_df = test[test['lambda'] == l1]\n",
    "\n",
    "    # # Find the gamma value where eBIC is the smallest\n",
    "    min_gamma = test.loc[test['eBIC'].idxmin(), 'gamma']\n",
    "    lambda_list.append(l1)\n",
    "    gamma_list.append(min_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gamma_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = result_df[result_df[\"solver\"] == 'sgl']\n",
    "\n",
    "# Find the lambda and gamma values with the lowest eBIC for each solver\n",
    "min_eBIC_values = test.groupby([\"lambda\"])['eBIC'].min()\n",
    "min_eBIC_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test[\"lambda\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to store coordinates of the minimum eBIC value\n",
    "for solver in result_df[\"solver\"].unique():\n",
    "    df = result_df[result_df[\"solver\"] == solver]\n",
    "    # Create heatmap plot\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        x=np.log(df['lambda']),\n",
    "        y=df['gamma'],\n",
    "        z=np.log(df['eBIC']),\n",
    "        colorscale='Viridis',\n",
    "        colorbar=dict(title='log(eBIC)')\n",
    "    ))\n",
    "    # Set labels and title\n",
    "    fig.update_layout(\n",
    "        title=solver,\n",
    "        xaxis_title='log(lambda)',\n",
    "        yaxis_title='gamma',\n",
    "        width=900,  # Set the width of the plot (in pixels)\n",
    "        height=600  # Set the height of the plot (in pixels)\n",
    "    )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for solver in result_df[\"solver\"].unique():\n",
    "    df = result_df[result_df[\"solver\"] == solver]\n",
    "    # Create heatmap plot\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        x=np.log(df['lambda']),\n",
    "        y=df['gamma'],\n",
    "        z=np.log(df['eBIC']),\n",
    "        colorscale='Viridis',\n",
    "        colorbar=dict(title='log(eBIC)')\n",
    "    ))\n",
    "    # Set labels and title\n",
    "    fig.update_layout(\n",
    "        title = solver,\n",
    "        xaxis_title='log(lambda)',\n",
    "        yaxis_title='gamma',\n",
    "        width=900,  # Set the width of the plot (in pixels)\n",
    "        height=600  # Set the height of the plot (in pixels)\n",
    "    )\n",
    "\n",
    "    gamma_list = []\n",
    "    lambda_list = []\n",
    "\n",
    "    for l1 in test['lambda'].unique():\n",
    "        # Filter the DataFrame for lambda = 1\n",
    "        filtered_df = test[test['lambda'] == l1]\n",
    "\n",
    "        # # Find the gamma value where eBIC is the smallest\n",
    "        min_gamma = test.loc[test['eBIC'].idxmin(), 'gamma']\n",
    "        lambda_list.append(l1)\n",
    "        gamma_list.append(min_gamma)\n",
    "        \n",
    "    \n",
    "    # Mark the minimum eBIC value with a dot\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=np.log(lambda_list),\n",
    "        y=gamma_list,\n",
    "        mode='lines',\n",
    "        marker=dict(\n",
    "            size=20,\n",
    "            color='red'\n",
    "        ),\n",
    "        showlegend=False\n",
    "    ))\n",
    "    # Add annotation\n",
    "    fig.add_annotation(\n",
    "        x=np.log(lambda_list[5]),\n",
    "        y=gamma_list[0],\n",
    "        text=\"Minimum eBIC\",\n",
    "        font=dict(size=18, color =\"white\"),\n",
    "        ax=-10,\n",
    "        ay=-10\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    # Show the plot\n",
    "    fig.write_image(\"plots/{0}_ebic_heatmap.png\".format(solver))\n",
    "    fig.write_image(\"plots/{0}_ebic_heatmap.pdf\".format(solver))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for solver in result_df[\"solver\"].unique():\n",
    "    df = result_df[result_df[\"solver\"] == solver]\n",
    "    # Create heatmap plot\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        x=df['lambda'],\n",
    "        y=df['gamma'],\n",
    "        z=df['SP'],\n",
    "        colorscale='Viridis',\n",
    "        colorbar=dict(title='SP')\n",
    "    ))\n",
    "    # Set labels and title\n",
    "    fig.update_layout(\n",
    "        title = solver,\n",
    "        xaxis_title='lambda',\n",
    "        yaxis_title='gamma',\n",
    "        width=900,  # Set the width of the plot (in pixels)\n",
    "        height=600  # Set the height of the plot (in pixels)\n",
    "    )\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "    fig.write_image(\"plots/{0}_SP_heatmap.png\".format(solver))\n",
    "    fig.write_image(\"plots/{0}_SP_heatmap.pdf\".format(solver))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key in P_SGL.modelselect_stats['BIC'].keys():\n",
    "    x = P_SGL.modelselect_stats['BIC'][key]\n",
    "    y = P_SGL.modelselect_stats['LAMBDA']\n",
    "\n",
    "    trace = go.Scatter(x=y.flatten(), y=x.flatten(), mode='lines')\n",
    "    data = [trace]\n",
    "\n",
    "    layout = go.Layout(title='Lambda path vs. eBIC ({0})'.format(key), xaxis_title='LAMBDA', yaxis_title='log(eBIC)', yaxis_type='log')\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = P_SGL.modelselect_stats['SP']\n",
    "y = P_SGL.modelselect_stats['LAMBDA']\n",
    "\n",
    "trace = go.Scatter(x=y.flatten(), y=x.flatten(), mode='lines')\n",
    "data = [trace]\n",
    "\n",
    "layout = go.Layout(title='Lambda path vs. Sparsity', xaxis_title='LAMBDA', yaxis_title='sparsity')\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = P_SGL_low.modelselect_stats['BIC'][0.1][:, 0]\n",
    "y = P_SGL_low.modelselect_stats['LAMBDA'][:, 0]\n",
    "\n",
    "trace = go.Scatter(x=y.flatten(), y=x.flatten(), mode='lines')\n",
    "data = [trace]\n",
    "\n",
    "layout = go.Layout(title='Lambda path vs. eBIC', xaxis_title='LAMBDA', yaxis_title='log(eBIC)', yaxis_type='log')\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = P_SGL_low.modelselect_stats['SP'][:, 0]\n",
    "y = P_SGL_low.modelselect_stats['LAMBDA'][:, 0]\n",
    "\n",
    "trace = go.Scatter(x=y.flatten(), y=x.flatten(), mode='lines')\n",
    "data = [trace]\n",
    "\n",
    "layout = go.Layout(title='Lambda path vs. Sparsity', xaxis_title='LAMBDA', yaxis_title='sparsity')\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "P_SGL_low.modelselect_stats['SP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = P_SGL_low.solution.lowrank_\n",
    "r = np.linalg.matrix_rank(L)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_SGL_low.modelselect_stats['RANK'].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "P_SGL_low.modelselect_stats\n",
    "# P_SGL.modelselect_stats['LAMBDA']\n",
    "# P_SGL.modelselect_stats['MU']\n",
    "# P_SGL.modelselect_stats['SP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 1500\n",
    "height = 1500\n",
    "label_size = \"8pt\"\n",
    "\n",
    "# for visualization reasons we transform inverse covaraince to negative inverse covaraince, i.e., multiply by -1\n",
    "sgl = -1 * pd.DataFrame(P_SGL.solution.precision_, columns=asv_names, index=asv_names)\n",
    "adapt = -1 * pd.DataFrame(P_SGL_adapt.solution.precision_, columns=vis_df.columns, index=vis_df.columns)\n",
    "low = -1 * pd.DataFrame(P_SGL_low.solution.precision_, columns=asv_names, index=asv_names)\n",
    "\n",
    "\n",
    "lables_sgl, re_labels_sgl = create_label_dict(sgl)\n",
    "lables_adapt, re_labels_adapt = create_label_dict(adapt)\n",
    "lables_low, re_labels_low = create_label_dict(low)\n",
    "\n",
    "p_sgl = _make_heatmap(data=sgl, labels_dict=lables_sgl, labels_dict_reversed=re_labels_sgl, # multiply by 3 for making edge visible on the heatmao\n",
    "                       title=\"SGL estimated (negative) inverse covariance\", width=width, height=height,\n",
    "                       label_size=label_size)\n",
    "\n",
    "p_adapt = _make_heatmap(data=adapt, labels_dict=lables_adapt, labels_dict_reversed=re_labels_adapt,\n",
    "                       title=\"Adaptive estimated (negative) inverse covariance\", width=width, height=height,\n",
    "                       label_size=label_size)\n",
    "\n",
    "p_low = _make_heatmap(data=low, labels_dict=lables_low, labels_dict_reversed=re_labels_low,\n",
    "                       title=\"SGL+low-rank estimated (negative) inverse covariance\", width=width, height=height,\n",
    "                       label_size=label_size)\n",
    "show(p_sgl)\n",
    "show(p_adapt)\n",
    "show(p_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# g_sgl = sns.clustermap(sgl, method='average', cmap='RdBu', center=0, dendrogram_ratio=0.2, robust=True, cbar_pos=None)\n",
    "\n",
    "# # get the order of the rows and columns\n",
    "# row_order_sgl = g_sgl.dendrogram_row.reordered_ind\n",
    "# col_order_sgl = g_sgl.dendrogram_col.reordered_ind\n",
    "\n",
    "sgl_clust = sgl.iloc[row_order_vis_S, col_order_vis_S]\n",
    "\n",
    "lables_sgl_clust, re_labels_sgl_clust = create_label_dict(sgl_clust)\n",
    "\n",
    "p_sgl_clust = _make_heatmap(data=sgl_clust, labels_dict=lables_sgl_clust, labels_dict_reversed=re_labels_sgl_clust, # multiply by 3 for making edge visible on the heatmao\n",
    "                       title=\"Clustered SGL estimated (negative) inverse covariance\", width=width, height=height,\n",
    "                       label_size=label_size)\n",
    "\n",
    "show(p_sgl_clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = dict(zip(row_order_vis_S, dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "asv_part_adapt = adapt.iloc[:-n_cov, :-n_cov].iloc[row_order_vis_S, col_order_vis_S]\n",
    "\n",
    "asv_part_adapt.columns\n",
    "\n",
    "covs_adapt = adapt.iloc[:-n_cov, -n_cov:].iloc[row_order_vis_S, :]\n",
    "covs_adapt\n",
    "\n",
    "only_cov = adapt.iloc[-n_cov:, -n_cov:]\n",
    "\n",
    "a = asv_part_adapt.values\n",
    "b = covs_adapt.values\n",
    "c = only_cov.values\n",
    "\n",
    "# Combine arrays a, b_reshaped, and c_reshaped along the last axis (axis=-1)\n",
    "res = np.block([\n",
    "    [a, b],\n",
    "    [b.T, c]])\n",
    "\n",
    "adapt_clust = pd.DataFrame(res, index=list(asv_part_adapt.columns) + list(only_cov.columns), columns=list(asv_part_adapt.columns) + list(only_cov.columns))\n",
    "adapt_clust\n",
    "# lables_adapt_clust, re_labels_adapt_clust = create_label_dict(adapt_clust)\n",
    "\n",
    "# p_adapt_clust = _make_heatmap(data=adapt_clust, labels_dict=lables_adapt_clust, labels_dict_reversed=re_labels_adapt_clust, # multiply by 3 for making edge visible on the heatmao\n",
    "#                        title=\"Clustered Adaptive SGL estimated (negative) inverse covariance\", width=width, height=height,\n",
    "#                        label_size=label_size)\n",
    "\n",
    "# show(p_adapt_clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# g_lsp = sns.clustermap(low, method='average', cmap='RdBu', center=0, dendrogram_ratio=0.2, robust=True, cbar_pos=None)\n",
    "\n",
    "# # get the order of the rows and columns\n",
    "# row_order_lsp = g_lsp.dendrogram_row.reordered_ind\n",
    "# col_order_lsp = g_lsp.dendrogram_col.reordered_ind\n",
    "\n",
    "lsp_clust = low.iloc[row_order_vis_S, col_order_vis_S]\n",
    "\n",
    "lables_lsp_clust, re_labels_lsp_clust = create_label_dict(lsp_clust)\n",
    "\n",
    "p_lsp_clust = _make_heatmap(data=lsp_clust, labels_dict=lables_lsp_clust, labels_dict_reversed=re_labels_lsp_clust, # multiply by 3 for making edge visible on the heatmao\n",
    "                       title=\"Clustered SGL estimated (negative) inverse covariance\", width=width, height=height,\n",
    "                       label_size=label_size)\n",
    "\n",
    "show(p_lsp_clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lrp = -1 * pd.DataFrame(P_SGL_low.solution.lowrank_, columns=asv_names, index=asv_names)\n",
    "\n",
    "# g_lrp = sns.clustermap(lrp, method='average', cmap='RdBu', center=0, dendrogram_ratio=0.2, robust=True, cbar_pos=None)\n",
    "\n",
    "# # get the order of the rows and columns\n",
    "# row_order_lrp = g_lrp.dendrogram_row.reordered_ind\n",
    "# col_order_lrp = g_lrp.dendrogram_col.reordered_ind\n",
    "\n",
    "lrp_clust = lrp.iloc[row_order_vis_S, col_order_vis_S]\n",
    "\n",
    "lables_lrp_clust, re_labels_lrp_clust = create_label_dict(lrp_clust)\n",
    "\n",
    "p_sgl_lrp = _make_heatmap(data=lrp_clust, labels_dict=lables_lrp_clust, labels_dict_reversed=re_labels_lrp_clust, # multiply by 3 for making edge visible on the heatmao\n",
    "                       title=\"Clustered Low Rank\", width=width, height=height,\n",
    "                       label_size=label_size)\n",
    "\n",
    "show(p_sgl_lrp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### for 0.9\n",
    "meta_cols = list(adapt.iloc[:, -n_cov:].columns)\n",
    "asv18_edges = [\"ASV_18\", \"ASV_25\", \"ASV_27\", \"ASV_32\", \"ASV_34\"]\n",
    "asv25_32 =[\"ASV_25\", \"ASV_32\"]\n",
    "asv18_27 =[\"ASV_18\", \"ASV_27\"]\n",
    "asv18_edges_adapt = ['ph','average-soil-temperature', 'average-soil-relative-humidity'] + asv18_27\n",
    "# asv18_edges_adapt = ['average-soil-temperature'] + asv18_edges\n",
    "\n",
    "sgl_edges = sgl[sgl.columns.intersection(asv18_edges)].loc[asv18_edges]\n",
    "adapt_edges = adapt[adapt.columns.intersection(asv18_edges_adapt)].loc[asv18_edges_adapt]\n",
    "low_edges = low[low.columns.intersection(asv18_edges)].loc[asv18_edges]\n",
    "\n",
    "\n",
    "G_SGL = create_graph(sgl_edges, threshold=0.0)\n",
    "G_adapt = create_graph(adapt_edges, threshold=0.0)\n",
    "G_low = create_graph(low_edges, threshold=0.0)\n",
    "\n",
    "\n",
    "# G_SGL = create_graph(sgl, threshold=0.01)\n",
    "# G_adapt = create_graph(adapt, threshold=0.01)\n",
    "# G_low = create_graph(low, threshold=0.01)\n",
    "\n",
    "\n",
    "width, height= 1000, 1000\n",
    "\n",
    "network_sgl = plot_network(G_SGL, title=\"SGL\", height=height, width=width, amplify_x=10)\n",
    "network_adapt = plot_network(G_adapt, title=\"Adaptive\",  height=height, width=width, amplify_x=10)\n",
    "network_low = plot_network(G_low, title=\"Low-rank\",  height=height, width=width, amplify_x=10)\n",
    "\n",
    "show(network_sgl)\n",
    "show(network_adapt)\n",
    "show(network_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### for 0.8\n",
    "# meta_cols = list(adapt.iloc[:, -n_cov:].columns)\n",
    "# asv18_edges = [\"ASV_3\", \"ASV_8\", \"ASV_9\", \"ASV_13\"]\n",
    "# # asv18_51 =[\"ASV_6\", \"ASV_17\", \"ASV_20\"]\n",
    "# # asv18_edges_adapt = ['ph','average-soil-temperature', 'average-soil-relative-humidity'] + asv18_edges\n",
    "# asv18_edges_adapt = ['average-soil-temperature'] + asv18_edges\n",
    "\n",
    "# sgl_edges = sgl[sgl.columns.intersection(asv18_edges)].loc[asv18_edges]\n",
    "# adapt_edges = adapt[adapt.columns.intersection(asv18_edges_adapt)].loc[asv18_edges]\n",
    "# low_edges = low[low.columns.intersection(asv18_edges)].loc[asv18_edges]\n",
    "\n",
    "\n",
    "# G_SGL = create_graph(sgl_edges, threshold=0.0)\n",
    "# G_adapt = create_graph(adapt_edges, threshold=0.0)\n",
    "# G_low = create_graph(low_edges, threshold=0.0)\n",
    "\n",
    "\n",
    "# # G_SGL = create_graph(sgl, threshold=0.01)\n",
    "# # G_adapt = create_graph(adapt, threshold=0.01)\n",
    "# # G_low = create_graph(low, threshold=0.01)\n",
    "\n",
    "\n",
    "# width, height= 1000, 1000\n",
    "\n",
    "# network_sgl = plot_network(G_SGL, title=\"SGL\", height=height, width=width, amplify_x=50)\n",
    "# network_adapt = plot_network(G_adapt, title=\"Adaptive\",  height=height, width=width, amplify_x=10)\n",
    "# network_low = plot_network(G_low, title=\"Low-rank\",  height=height, width=width, amplify_x=50)\n",
    "\n",
    "# show(network_sgl)\n",
    "# show(network_adapt)\n",
    "# show(network_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# p_18_51 = scater_plot(vis_df[\"ASV_27\"], vis_df[\"ASV_18\"])\n",
    "p_18_temp = scater_plot(vis_df[\"ASV_27\"], meta['ph'].loc[df.index])\n",
    "p_51_temp = scater_plot(vis_df[\"ASV_18\"], meta['ph'].loc[df.index])\n",
    "\n",
    "# show(p_18_51)\n",
    "show(p_18_temp)\n",
    "show(p_51_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asv.shape, L_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_cov = adapt.iloc[:-n_cov, -n_cov:]\n",
    "\n",
    "L_adapt = inv_cov @ inv_cov.T\n",
    "L_adapt.shape\n",
    "\n",
    "L_1 = pd.DataFrame(P_SGL_low.solution.lowrank_, columns=asv_names, index=asv_names)\n",
    "L_2 = pd.DataFrame(L_adapt, columns=asv_names, index=asv_names)\n",
    "\n",
    "r1 = np.linalg.matrix_rank(L_1)\n",
    "r2 = np.linalg.matrix_rank(L_2)\n",
    "\n",
    "print(\"L1-rank: {0}\".format(r1))\n",
    "print(\"L2-rank: {0}\".format(r2))\n",
    "\n",
    "proj_1, loadings_1, eigv_1 = PCA(asv, L_1, inverse=True)\n",
    "\n",
    "eigv_sum_1 = np.sum(eigv_1)\n",
    "var_exp_1 = [(value / eigv_sum_1) for value in sorted(eigv_1, reverse=True)]\n",
    "\n",
    "proj_2, loadings_2, eigv_2 = PCA(asv, L_2, inverse=True)\n",
    "\n",
    "eigv_sum_2 = np.sum(eigv_2)\n",
    "var_exp_2 = [(value / eigv_sum_2) for value in sorted(eigv_2, reverse=True)]\n",
    "\n",
    "pca_plot = project_covariates(asv, metadata=meta, L=L_1, y='average-soil-temperature')\n",
    "# pca_plot = project_covariates(asv, metadata=meta, L=L_1, y='ph', PC=0)\n",
    "# pca_plot = project_covariates(asv, metadata=meta, L=L_1, y='PC2', PC=0)\n",
    "# pca_plot = project_covariates(asv, metadata=meta, L=L_1, y='ph')\n",
    "show(pca_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project PCs on ASVs and covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pc_1 = pd.Series(proj_1[:, 0], index=asv.index, name='PC1')\n",
    "\n",
    "p_18_temp = scater_plot(vis_df[\"ASV_27\"], pc_1)\n",
    "p_51_temp = scater_plot(vis_df[\"ASV_18\"], pc_1)\n",
    "\n",
    "show(p_18_temp)\n",
    "show(p_51_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_temp = scater_plot(pc_1, meta[\"average-soil-temperature\"].loc[pc_1.index])\n",
    "\n",
    "show(p_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 1500\n",
    "height = 1500\n",
    "label_size = \"8pt\"\n",
    "\n",
    "adapt_theta = adapt.copy()\n",
    "\n",
    "asv_cov = adapt_theta.iloc[:-n_cov, -n_cov:]\n",
    "\n",
    "l1_norm = np.linalg.norm(asv_cov.values, axis=1)\n",
    "\n",
    "adapt_theta['l1'] = np.append(l1_norm, np.zeros(n_cov))\n",
    "\n",
    "adapt_theta = adapt_theta.T\n",
    "\n",
    "adapt_theta['l1'] = np.append(l1_norm, np.zeros(n_cov+1))\n",
    "adapt_theta = adapt_theta.sort_values(by=['l1'], ascending=False)\n",
    "adapt_theta = adapt_theta.T\n",
    "adapt_theta = adapt_theta.sort_values(by=['l1'], ascending=False)\n",
    "\n",
    "lables_l1, re_labels_l1 = create_label_dict(adapt_theta)\n",
    "\n",
    "p_l1 = _make_heatmap(data=adapt_theta, labels_dict=lables_l1, labels_dict_reversed=re_labels_l1,\n",
    "                       title=\"Esatimated inverse covariance sorted by l1-norm of the covariates\", width=width, height=height,\n",
    "                       label_size=label_size)\n",
    "show(p_l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_order = adapt_theta.index[:-n_cov-1].values\n",
    "sorted_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "width = 1500\n",
    "height = 1500\n",
    "label_size = \"8pt\"\n",
    "\n",
    "# for visualization reasons we transform inverse covaraince to negative inverse covaraince, i.e., multiply by -1\n",
    "sgl = -1 * pd.DataFrame(P_SGL.solution.precision_, columns=asv_names, index=asv_names)\n",
    "adapt = -1 * pd.DataFrame(P_SGL_adapt.solution.precision_, columns=vis_df.columns, index=vis_df.columns)\n",
    "low = -1 * pd.DataFrame(P_SGL_low.solution.precision_, columns=asv_names, index=asv_names)\n",
    "\n",
    "\n",
    "sorted_sgl = sgl.reindex(index=sorted_order).T.reindex(index=sorted_order).T\n",
    "sorted_low = low.reindex(index=sorted_order).T.reindex(index=sorted_order).T\n",
    "\n",
    "sorted_lables_sgl, sorted_re_labels_sgl = create_label_dict(sorted_sgl)\n",
    "sorted_lables_low, sorted_re_labels_low = create_label_dict(sorted_low)\n",
    "\n",
    "sorted_p_sgl = _make_heatmap(data=sorted_sgl, labels_dict=sorted_lables_sgl, labels_dict_reversed=sorted_re_labels_sgl, # multiply by 3 for making edge visible on the heatmao\n",
    "                       title=\"SGL estimated (negative) inverse covariance sorted by l1\", width=width, height=height,\n",
    "                       label_size=label_size)\n",
    "\n",
    "sorted_p_low = _make_heatmap(data=sorted_low, labels_dict=sorted_lables_low, labels_dict_reversed=sorted_re_labels_low,\n",
    "                       title=\"SGL+low-rank estimated (negative) inverse covariance sorted by l1\", width=width, height=height,\n",
    "                       label_size=label_size)\n",
    "show(sorted_p_sgl)\n",
    "show(sorted_p_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_components = pd.DataFrame(loadings_1, index=low.index)\n",
    "pc_components = pc_components.iloc[::-1]\n",
    "pc_components.columns = [\"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\", \"PC6\"]\n",
    "\n",
    "# low-rank solution: r1=6\n",
    "identity = pd.DataFrame(np.eye(r1, r1), index=pc_components.columns, columns = pc_components.columns)\n",
    "# PCs are linearly independent by the definition\n",
    "pc_columns = pd.concat([pc_components, identity], axis=0)\n",
    "\n",
    "# inverse cov matrix extended by PCs\n",
    "asv_pc = pd.concat([low, pc_components], axis=1)\n",
    "asv_pc = pd.concat([asv_pc.T, pc_columns], axis=1)\n",
    "\n",
    "asv_low = asv_pc.iloc[:-r1, -r1:]\n",
    "# l1-norm of partial correlation between ASVs and PCs\n",
    "l1_norm_pc = np.linalg.norm(asv_low.values, axis=1)\n",
    "\n",
    "asv_pc['l1'] = np.append(l1_norm_pc, np.zeros(r1))\n",
    "asv_pc = asv_pc.T\n",
    "asv_pc['l1'] = np.append(l1_norm_pc, np.zeros(r1 + 1))\n",
    "\n",
    "#sorting by the order of adaptive l1-norm sorted solution\n",
    "n_asvs = len(vis_S)\n",
    "sorted_asv = asv_pc.iloc[:n_asvs, :].reindex(index=adapt_theta.iloc[:n_asvs, :].index)\n",
    "sorted_asv_pc = sorted_asv.T.join(asv_pc.iloc[:, -7:])\n",
    "sorted_asv = sorted_asv_pc.iloc[:n_asvs, :].reindex(index=adapt_theta.iloc[:n_asvs, :].index)\n",
    "sorted_l1_low = pd.concat([sorted_asv, sorted_asv_pc.iloc[n_asvs:, :]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lables_l1_low, re_labels_l1_low = create_label_dict(sorted_l1_low)\n",
    "\n",
    "p_l1_low = _make_heatmap(data=sorted_l1_low, labels_dict=lables_l1_low, labels_dict_reversed=re_labels_l1_low,\n",
    "                       title=\"Esatimated inverse covariance (sparse + low-rank) sorted by l1-norm of the PCs\", width=width, height=height,\n",
    "                       label_size=label_size)\n",
    "show(p_l1_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 1500\n",
    "height = 1500\n",
    "label_size = \"8pt\"\n",
    "\n",
    "# for visualization reasons we transform inverse covaraince to negative inverse covaraince, i.e., multiply by -1\n",
    "sgl = -1 * pd.DataFrame(P_SGL.solution.precision_, columns=asv_names, index=asv_names)\n",
    "adapt = -1 * pd.DataFrame(P_SGL_adapt.solution.precision_, columns=vis_df.columns, index=vis_df.columns)\n",
    "low = -1 * pd.DataFrame(P_SGL_low.solution.precision_, columns=asv_names, index=asv_names)\n",
    "\n",
    "\n",
    "lables_sgl, re_labels_sgl = create_label_dict(sgl)\n",
    "lables_adapt, re_labels_adapt = create_label_dict(adapt)\n",
    "lables_low, re_labels_low = create_label_dict(low)\n",
    "\n",
    "p_sgl = _make_heatmap(data=sgl, labels_dict=lables_sgl, labels_dict_reversed=re_labels_sgl,\n",
    "                       title=\"SGL estimated (negative) inverse covariance\", width=width, height=height,\n",
    "                       label_size=label_size)\n",
    "\n",
    "p_adapt = _make_heatmap(data=adapt, labels_dict=lables_adapt, labels_dict_reversed=re_labels_adapt,\n",
    "                       title=\"Adaptive estimated (negative) inverse covariance\", width=width, height=height,\n",
    "                       label_size=label_size)\n",
    "\n",
    "p_low = _make_heatmap(data=low, labels_dict=lables_low, labels_dict_reversed=re_labels_low,\n",
    "                       title=\"SGL+low-rank estimated (negative) inverse covariance\", width=width, height=height,\n",
    "                       label_size=label_size)\n",
    "show(p_sgl)\n",
    "show(p_adapt)\n",
    "show(p_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_cols = list(adapt.iloc[:, -n_cov:].columns)\n",
    "\n",
    "asv69_edges = [\"ASV_18\", \"ASV_51\", \"ASV_46\", \"ASV_13\", \"ASV_7\", \"ASV_5\"]\n",
    "asv69_edges_adapt = meta_cols + [\"ASV_18\", \"ASV_51\", \"ASV_5\", \"ASV_46\"]\n",
    "asv69_edges_low = asv69_edges\n",
    "\n",
    "sgl_edges = sgl[sgl.columns.intersection(asv69_edges)].loc[asv69_edges]\n",
    "adapt_edges = adapt[adapt.columns.intersection(asv69_edges_adapt)].loc[asv69_edges_adapt]\n",
    "low_edges = low[low.columns.intersection(asv69_edges_low)].loc[asv69_edges_low]\n",
    "\n",
    "\n",
    "G_SGL = create_graph(sgl_edges, threshold=0.01)\n",
    "G_adapt = create_graph(adapt_edges, threshold=0.01)\n",
    "G_low = create_graph(low_edges, threshold=0.01)\n",
    "\n",
    "\n",
    "width, height= 1000, 1000\n",
    "\n",
    "network_sgl = plot_network(G_SGL, title=\"SGL\", height=height, width=width)\n",
    "network_adapt = plot_network(G_adapt, title=\"Adaptive\",  height=height, width=width)\n",
    "network_low = plot_network(G_low, title=\"Low-rank\",  height=height, width=width)\n",
    "\n",
    "show(network_sgl)\n",
    "show(network_adapt)\n",
    "show(network_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
